2015-09-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/op/loss.jl: TODO: accept sparse target.

	* src/param.jl: TODO: rename to Param.

2015-08-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* FIXED: /home/nlg-02/data07/eng/google-ngram/CompoundWords.dbg:
	julia my_train_and_random_test.3.jl train.data.part test.data.1
	gives illegal memory access error.  This was an issue with 32 vs
	64 bit integer indices with sparse matrices.

2015-08-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* RNN:
	- an rnn is still a vector of micro-layers.
	- we need two new layers, adder and elementwise multiplier for lstm?
	- each layer (except the adder) has a single input
	- the input is specified by two indices:
	-- index into rnn (absolute?), index into time (relative)
	- the adder can have a list of indices. (should we allow all to
	have this?)
	- x, y become a vector of matrices rather than a single matrix
	- we may want to use different variables for backward
	compatibility: xlist ylist?
	- dx, dy?
	- resetting the vectors?
	- feeding the input?
	- backprop?

2015-08-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO: implement alternatives to adagrad: adam, rmsprop,
	adadelta. test on bparser.

	* TODO: impement RNNs.

	* TODO: implement maxnorm, norm clipping etc.

	* TODO: doc before release:
	- Perceptron.
	- Layers.
	- Update (including averaging etc.)
	- ScalLoss?

2015-08-02  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/bias.jl: TODO: Create a @gpu macro.  Work on non-gpu compatibility.

2015-07-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/kperceptron.jl:
	- klinear != perceptron, averaging difference?
	+ klinear,single,sparse,cpu gets stuck: changed slow algorithm.

	* src/util/sparse.jl: Frequent problem I have been having is due
	to the following: when a function parameter has a parametrized
	type like KUsparse{Array}, that is parametrized by another type,
	it is better to make the inner type a variable.  i.e. instead of
	foo(a::KUsparse{Array}) use foo{A<:Array}(a::KUsparse{A})

2015-07-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/util/linalg.cu (_A_mul_Bs_32): DONE: Test this.

	* src/mmul.jl: DONE:
	+ not using averaging for prediction.
	+ cudaarray is not giving the same answer as array
	+ sparse does not work

	* src/net.jl: conversions in train and predict: do we end up with
	the same array type we started with?

2015-07-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/tutorial.jl: profile, did it get slower?

	* DONE:
	+ test mmul with dense and sparse
	+ fix kperceptron, perceptron
	+ make layers generic, support regular arrays, if possible
	in-place sparse arrays.
	+ unit test cadd!

	* src/util/linalg.jl: DONE:
	+ add linalg tests to dense and sparse.

2015-07-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/testsparse.jl (density): DONE:
	+ implement full
	+ type cannot be constructed: use ::Type{T} notation.
	+ implement uniq! for KUsparse

	* test/testdense.jl: DONE: add colops and linalg? tests.  Write
	testsparse.

2015-07-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* BUGS:
	+ cpucopy/gpucopy: fixed
	- load/save: doubles the storage.
	- savenet is failing.
	- reshape is missing.
	- cpucopy leaves KUparam{CudaArray}
	- cpucopy does not work with net
	- savenet causes seg fault
	- test for cpu only machine
	- introduce update_list and skip_list
	+ turn all cu params into double

	* layers: TODO: Layers are functions.  They should be generic.
	Their output types should match their input types.  Internally
	they can do whatever they want.  If the input type does not match
	the parameters the parameters should be moved?  No, better if only
	flexible at init.  We should support regular arrays not just
	KUarray's for input/output.  KUarray's just make certain
	operations faster, so train/predict can use them during
	minibatching.  We should specify the array types supported by each
	layer.

2015-07-16  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/dense.jl: ok, we have three types of arrays: sparse, dense,
	param.  These have different members, so they can't just be
	parametrizations of a common type.  However each of these has
	array type, element type and dimensions as parameters.

	* src/bias.jl: should rename param for consistency.  data->arr.
	Define GPUparam and CPUparam?

	* src/mmul.jl: where do we need extensible arrays?  x, y,
	dataflow.  Train batches.  Not w params.  Except kernel perceptron
	support vectors and weights.  If we used regular arrays for
	params, load/save problem would go away.  Do we replace each l.x
	with l.x.arr?  Or do we define ops that work on KUarrays?  Then we
	need to consider mixtures.  Best to just work with your own arrays...

	OK, best of both worlds: we have two types of arrays, Param and
	Data arrays.  They have different requirements.  Just define the
	necessary functions for each.  Data arrays are resizeable whereas
	Param arrays have derivatives and associated update parameters.

2015-07-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* ARRAYS:
	We want to support cpu/gpu, sparse/dense, float32/64, 2D/4D dynamic
	(efficiently extendible) array types in KUnet.  The relevant part of
	the current array hierarchy in Julia is as follows:

	AbstractArray
	  DenseArray
	    Array
	  AbstractSparseArray
	    SparseMatrixCSC
	  SubArray

	The CUDArt hierarchy is disconnected:

	AbstractCudaArray
	  CudaArray
	  CudaPitchedArray
	AbstractArray
	  HostArray

	CUDArt also defines two convenience types:

	typealias CdArray{T} Union(DenseArray{T},HostArray{T},AbstractCudaArray{T})
	typealias ContiguousArray{T} Union(Array{T},HostArray{T},CudaArray{T})

	Notes:

	So far I have added CudaDynArray and CudaSparseMatrixCSC to this
	list.

	CPU Arrays currently do not handle resize and hcat efficiently.
	They probably need a special type as well.  We might as well
	define all 4 types.

	Sparse arrays are needed only as inputs and possibly support vector
	matrices.  Things quickly turn dense after that.

	-- Layers Should Adapt to Their Inputs --
	Declaring a model with specific input output array types, vs
	declaring an abstract model which then takes the appropriate shape
	when first batch of data is seen.  Certainly the abstract
	declaration has an ease of use advantage, the constructions are
	more readable.  The more abstract the model code the better.
	However this means a lot of low level initialization code goes
	into layer definitions (initforw etc.).  What if input type
	changes during the lifetime of the model?  We adapt to size
	changes, why not type changes? eltype changes?  Device, sparsity,
	array implementation (there could be more than one sparse array
	type), eltype, ndims: layers should adapt to their input and be
	able to change (maybe with a warning) midstream.  The output types
	(both y and dx) should match the input types (except y may not
	match x in ndims.)

	Operations to be supported:
	train: shufflexy!, size, ndims, x2b
	predict: b2y
	layers: similar! (for changing type), resize (for changing size)
	kperceptron: hcat! (bparse also needs this)
	mmul: A_mul_B! A_mul_Bt!

	Each layer should document which array types supported.

	* TODO: organize code around operations (array.jl, linalg.jl) or
	data types (sparse, cusparse)?

	* TODO: release perceptron.
	+ do a pre-release
	- finish documentation.
	- implement structured perceptron? just a difft training rule?
	- test mmul+percloss, need averaging, implement ASGD.

	* TODO: nvidia bug check: cudnnSoftmaxBackward off by a factor of
	2? came back saying there is no bug.  also report the cusparse
	transpose problem.


2015-07-09  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO: automatic spawn of worker machines in qsub.  From JonMay:
	Once a job starts you will have the environment variable
	PBS_NODEFILE and this will contain a list of machines, one per
	line.

	e.g. (sorry, it happened to be a one-node job and i didnâ€™t want to
	wait for my 2-node demo to start)

	[jonmay@hpc1457 quake]$ echo $PBS_NODEFILE
	/var/spool/torque/aux//12554804.hpc-pbs.hpcc.usc.edu
	[jonmay@hpc1457 quake]$ cat
	/var/spool/torque/aux//12554804.hpc-pbs.hpcc.usc.edu
	hpc1457

2015-07-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/util.jl: TODO: use size! instead of realloc in similar!
	look at size! vs resize! again
	make sure regular arrays will work
	similar! does not match array type?  reconsider its semantics.

	* net.jl: profiling kunet.  nothing much to do.

	# train: most time spent copying data to gpu
	gpu() && gc()  # L46: 76
	for b = 1:batch:ninst # L47: 1
        e = min(ninst, b + batch - 1)
        xx = x2b(xx, x, b:e) # L49: 25641
        yy = x2b(yy, y, b:e) # L50: 39
        backprop(net, xx, yy; o...) # L51: 516
        update(net; o...) # L52: 160
        (iters > 0) && (e/batch >= iters) && break
        gpu() && (gpumem() < (1<<30)) && gc() # L54: 32
	end
	strip!(net)
	gpu() && gc()

	# predict: most time spent on copying back from gpu.
	ninst = size(x, ndims(x))
	(batch == 0 || batch > ninst) && (batch = ninst)
	xx = yy = nothing
	for b = 1:batch:ninst
        e  = min(ninst, b + batch - 1)
        xx = x2b(xx, x, b:e)  # L68: 382
        yy = forw(net, xx; predict=true, o...) # L69: 150
        y  = b2y(y, yy, b:e, x) # L70: 4150
	end
	return y


2015-07-03  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/simpleIPC/fooipc.cu (main): seems like the only choices are
	using lightweight threads or memcopy.  a memory handle seems
	sharable by only one other process?

	* src/cumatrix.jl: TODO: is it better to split code by function
	rather than datatype?  more similarity.

	* src/sparse.jl: DONE: better interface for hcat!, last two args
	should be optional.

	* test/runtests.jl: TODO: test different cpu/gpu sparse/dense
	float32/float64 combos.

	* src/util.jl: TODO: use CudaDynArray everywhere and minimize the realloc.

	* conv.jl: TODO: conv fails tutorial.jl code.  fix
	initialization: should figure out input size/type from first x
	like bias/mmul.

	* cusparse.jl: TODO: implement uniq! for sparse perceptron.

	* perceptron.jl: TODO: implement addx! for gpu perceptron.

	* usage.md: TODO: write loss documentation.

	* usage.md: TODO: write perceptron documentation.

	* param.jl: DONE: add initxavier for conv.jl

	* param.jl: TODO: fix adagrad initialization.

	* src/kperceptron.jl: TODO: make uniq! part of training somehow.

	* net.jl: profile train to see if gpumem hurts: it doesn't.  most
	of the time is spent during x2b.


2015-06-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/perceptron.jl: keep w the same orientation as x, makes
	easier to add.  Use w'*x like kperceptron.  Decided otherwise;
	wrong on both counts: w is dense, so not easier to add in either
	direction, and kperceptron holds it in w*k(x) position.

2015-06-23    <dyuret@ku.edu.tr>

	* src/kperceptron.jl: TODO: gpu/dense does not work with
	kpoly/kgauss yet.  gpu/sparse needs to be written.  Define new
	type CudaSparseMatrixCSC and test:
	- train (x2b,b2y)
	- initforw
	- forw
	- initback
	- back
	- update
	- hcat!
	- kpoly (gpu sparse/dense)
	- kgauss (gpu sparse/dense)

2015-06-22    <dyuret@ku.edu.tr>

	* DONE: xavier: https://github.com/BVLC/caffe/blob/master/include/caffe/filler.hpp#L129

2015-06-21    <dyuret@ku.edu.tr>

	* test/testkperceptron.jl:

	Q: linear kperceptron and perceptron do not give the same result?
	The difference is due to kperceptron not having a bias.  Removing
	bias from perceptron makes the results equal to numerical
	accuracy.

	Q: is K sparse or dense?  sparse.  +0 makes it dense.

	Q: should we add bias back to kperceptron but make it optional?
	it helps klinear.  does it help kpoly?  why does it hurt kgauss?

	Q: for loop is much slower than sparse mmul?

2015-06-20    <dyuret@ku.edu.tr>

	* kperceptron.jl: To move any further we need to sort out this
	array type business.  Since we introduced sparse arrays we are no
	longer limited to two types, Array and CudaArray.  That means the
	atype mechanism is no longer ok.

	First of all the original data comes in a cpu array.  It gets
	copied into minibatches by train and predict.  It can be full or
	sparse.  Nothing after the original data needs to be sparse except
	support vectors.  The minibatches could be on cpu or gpu (if gpu
	usage is specified).  We will keep the sparseness of the input in
	minibatches.  The users preferences should specify the cpu/gpu and
	the eltype that should be used internally in layer computations.
	Do we allow ftype to be different?  If not we could eliminate
	that and take it from the input as well?  So the input data
	determines the ftype and sparseness of layer calculations.  GPU
	used if present.  User has the option to turn gpu off.

	Conversions take place in param.jl, net.jl (train/predict).
	conv/pool only work with gpu arrays right now.
	perceptron/kperceptron only works with cpu arrays right now.

	So get rid of atype/ftype.  Get it from the input or during
	initialization.  So how do we initialize an mmul layer?  By
	specifying number of outputs.  Just like the perceptron.  The
	weight matrix gets constructed when the first input is received in
	initforw.  Mmul(array), Mmul(out), Mmul(out,in),
	Mmul(ftype,out,in) could be the initializers (modeled after
	zeros).  cpu/gpu is decided based on GPU setting (make it
	non-constant as before).  ftype defaults to Float64 as the rest of
	Julia (e.g. zeros(dims...), rand(dims...) etc).  So (out,in) can
	create a matrix.  Only (out) cannot, it will have to wait.  Param
	can play a more passive role?  Or we pass info to param?


2015-06-09    <dyuret@ku.edu.tr>

	* src/kernel.jl: Poly kernels working with sparse/full arrays.
	* DONE: Try on CudaArray: mul, xpose, hcat vs does not work.
	* DONE: Try on SparseCudaArray (cpu sparse is 5x slower than cpu
	full on mnist) -- postponed.
	* DONE: rbf kernel

2015-06-03    <dyuret@ku.edu.tr>

	* src/percloss.jl: Added perceptron loss.  A multiclass perceptron
	is just Mmul followed by PercLoss.
	* DONE: write unit test for PercLoss.
	* DONE: implement and test kernel perceptron next.
	* DONE: test subarray and concat with full/sparse on cpu/gpu.
	* DONE:	test KUnet with full/sparse, cpu/gpu, Float32/64 -- postponed
	* TODO: perform kuparser experiment comparing dense vs sparse
	features.
	* TODO: test onur's 4D code.
	* TODO: write doc on xent loss
	* TODO: write doc on perceptrons

2015-05-17    <dyuret@ku.edu.tr>

	* src/sigm.jl: done: make sure back returns dx.

	* src/logp.jl: done: gpu impl for back. why doesn't runtests catch
	this?  because dx=dy.

	* src/KUnet.jl: done: import copy etc.

	* src/net.jl:
	? add y=x and dx=dy optional args to forw and back.
	? the problem is do we copy if we don't modify?
	+ rename the options: fx=>dropout, dx=>returndx
	+ add o... options to all forw,back,copy,update,setparam!,loss

	* test/runtests.jl: done: cpu-only test.

	* test/runtests.jl: TODO: julia4 test.

	* src/param.jl: done: find a solution to copy.

2015-05-16    <dyuret@ku.edu.tr>

	* docs: TODO: update docs.

	* src/jldio.jl: done: update for new version.

	* src/net.jl: done: add shuffling back to train.

	* src/param.jl: done: compile cuda parts.

	* test/lenet.jl: done: need xavier init? the training does
	not take off until epoch 7 - turns out larger lr needed.

	* issimilar: done: add issimilar checks to all forw/back.

2015-05-15    <dyuret@ku.edu.tr>

	* test/runtests.jl: passed:
	+ bias.jl
	? conv.jl: only gpu, only 4D, no gradcheck
	+ drop.jl
	+ logp.jl
	+ logploss.jl
	+ mmul.jl
	? pool.jl: only gpu, only 4D, no gradcheck
	+ quadloss.jl
	+ relu.jl
	+ sigm.jl
	+ soft.jl
	+ softloss.jl
	+ tanh.jl
	+ xentloss.jl

	* src/xentloss.jl: Implementing loss functions as layers.  forw
	only records the outgoing y.  back takes the desired answer and
	overwrites it with the gradient of y wrt loss.  Note that this has
	two problems: (1) the actual loss value is never returned, i.e. we
	just compute gradients for training, use a separate loss function
	for eval. (2) the semantics of back is different: a regular
	layer's back takes dy, the loss gradient wrt output y, and returns
	dx, the loss gradient wrt input x.  A loss layer takes p, the gold
	answers, and returns dx, the loss gradient wrt input x.

2015-05-14    <dyuret@ku.edu.tr>

	* TODO:
	+ prob layer and loss fns: logploss, probloss, mseloss?, softmax?
	+ gradient check
	+ float64 support for Drop and Logp
	+ modify net.jl with the new loss convention.
	+ loss layers in gpu
	- conv pool gradcheck.
	- caffe comparison
	- conv pool cpu
	- conv pool 5D
	+ clean up float32:

	* test/runtests.jl: use Atype and Ftype to get gpu/cpu and
	float32/float64 behavior.

	* done: add a prob layer that computes normalized probabilities.
	Then rename three different softmax layers whether their input is
	unnormalized logp, logp, or prob.

2015-05-12    <dyuret@ku.edu.tr>

	* design: I have a new design:
	+ split every operation, including bias and activation.
	+ basically every operation in forw becomes its own "layer".
	+ each "layer" implements forw, back, update, setparam, copy.
	+ each "layer" overrides forw/back for arrays, cudaarrays, tensors.
	+ rename AbstractLayer -> Layer

2015-05-11    <dyuret@ku.edu.tr>

	* src/conv.jl:
	# TODO: How to transition from convolutional to fully connected layer?
	# Does a network pass around tensors with the same number of dimensions?
	# Can we write the code generic enough so it can deal with 2d matrices, 4d, 5d tensors?
	# In any case fc layer is different from conv layer...
	# n instances with c features is represented in caffe using a (1,1,C,N) tensor.
	# i.e. a 0-D image with C channels.
	# 2-D images are represented as (W,H,C,N).
	# 3-D images are represented as (D,W,H,C,N).
	# Is there any use for just (H,C,N)?
	# What is convolution for <2D or fc for >=2D?
	# Locality is important for convolution, i.e. dimensions other than
	# C,N give "neighboring" pixels so we can do convolution.
	# In a regular feature vector, there is no "neighbors" all features
	# are equally far from each other, that is why we use C,N.  There
	# would be no convolution operation in that case either.
	# N = instances in general (each instance leads to one class
	# prediction)
	# C = features in general
	# All other dimensions represent local neighborhoods.
	# So more generic data structure is reverse(N,C,I1,I2,I3,...)
	# And our regular layers are in fact 0-D, and conv can only be defined on >= 1-D.
	# We'd need to implement all except 2-D right now.
	# What does FC mean for >= 1-D?

	# TODO: do we really need the ConvolutionDescriptor

	# TODO: cudnn supports both Float32 and Float64, the rest of KUnet
	should too.

2015-03-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- Accept tuples for newnet and setparam to specify different
	values for different layers.  At least modify train.jl to be more
	similar to KUparser/test/train.jl.
	- sizeof, print for nets?
	- put an option for f of final layer (ffinal).
	+ add options to copy for testing (no need for training params)
	and saving (no need for transient fields).
	+ ERROR: CUDA runtime API library cannot be found - on yunus.
	- train.jl: allow length(v)==2*length(net) for param spec each w,b


2015-03-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- In KUnet, can we avoid reallocating everything unless we need more space?
	#    If batch gets smaller, just run the big batch through and copy part of the result?
	#    This needs some more thinking especially for training.


2015-03-01  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- make number type generic, test Float64
	- implement rmsprop: https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec6.pdf
	- implement adam: http://arxiv.org/pdf/1412.6980v2.pdf
	- understand adam math. what to do with g1?  what to do with g2?
	these are not stationary and our estimates are noisy.  what to do
	if we had perfect information?  does this correspond to newton
	with diagonal covariance matrix?  volkan's slides to adam email.
	- implement copynet cpu/gpu.
	- write parameter documentation.
	- implement hinge loss
	- implement squared loss
	- implement gradient clipping: pascanu and mikolov 2012.
	- implement rnn
	-- implement lstm
	-- implement gru
	-- steeper gates nips: lyu and zhu (piecewise linear)
	- orthogonal initialization: andrew sax
	- can we do piecewise linear approx to softmax? (hinge?)
	- try on machine with CUDArt but no gpu.

2015-02-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	+ start writing documentation.
	+ try install/test on a new gpu/cpu machine.
	- build tests based on mnist.
	- compare with matlab/caffe if they exist.
	- what other tests?  gradient?  store answers?

2015-02-23  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/KUnet.jl:
	+ reconsider the constructors: they should only allow meaningful
	fields to be set, and they should call setparam for updateparams.
	- implement convnet: ConvLayer <: Layer
	+ centralize memory allocation
	- hdf5 save for whole net: use jld?

2015-02-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- Make InplaceOps work without patch using CUBLAS generics.

2015-02-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:

	+ Write blogpost/README: deep learning in 250 lines of julia (needs mnist)
	- Write blogpost on overfitting (needs mnist)
	- Cleanup hdf5 files.
	- Figure out automatic testing.
	+ Make softloss, get rid of soft layer.
	- Add other losses
	+ make loss a training option.
	- Add sigmoid layer.
	+ Make b and yforw conditional?
	+ Figure out if we have a gpu and if we are using a gpu, test code on no-gpu machine
	+ Export functions
	+ Make layer constructors that take size and generate random matrices
	+ Make layer constructors that take arbitrary matrices, h5 files
	- Error checking in cuda.jl
	+ pull request for InplaceOps
	+ pull request for CUBLAS
	+ pull request for ArgParse
	- Cleanup kernel calls in kunet.cu
	- Have kernel softmax return loss?
	- Cleanup hdf5 format in kunet_h5.cu, get rid of xfunc, yfunc,
	+ make dropout a layer option.
	+ Make train call backprop
	- implement/test maxnorm?
	- implement/test maxout?
	- use mnist for regression testing.

2015-02-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- Verify generic functions in cuda.jl
	- Try to make update.jl more readable
	+ HDF5 should store the name of the functions
	+ Find a good way to handle dropout during training and testing.
	x maybe xforms should be part of the trainer not the layer.
	x caffe has it as another layer
	x i have tried as a separate fn or as part of forw/back before.

	+ implement/test dropout
	+ gpuseed not working, but random numbers start at the same place?
	  need to recreate RNG.
	+ cuda and julia not equal?
	x change dropout in cuda as well to use xmask for storage
	- change / check adagrad, nesterov options in cuda
	- implement/test maxnorm (cuda/matlab, no caffe test)

2015-02-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* DEBUG:
	+ test_fw_cuda: 2.26s
	+ test_fw_caffe: 3.82s
	+ test_fw_matlab: 3.83s
	+ test_fw_julia_cpu: 21.64s
	+ test_fw_julia_gpu: 5.39s ??? (check ger vs badd; do test with direct ccalls)
	+ who is allocating 35MB?
	+ elapsed time: 5.395230859 seconds (35 MB allocated, 0.06% gc time in 1 pauses with 0 full sweep)

2015-02-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	Possible design changes:
	+ Take training options out of Layer and pass them as options to layer update.
	+ That could support independent layer options but not w vs b.
	+ Group parameter and its diff in a blob like caffe: l->w->data, l->w->grad?
	x Make w and b two elements of an array: l->w[0,1]->ndims,dims,data,diff,diff1,diff2?
	x x and y have data,diff but no diff1 diff2.
	x But x has xmask, xones; we could use tmp1 and tmp2 as common names.
	+ Each w and b could have its own update options?
	+ Update can take each w, b individually, i.e. blob / options.
	x So can forward and back, take matrices instead of layers, but that's pushing it.
	+ To simplify memory management rely on xcols being correct in forw/drop and centralize alloc changes.

	+ figure out cuda rand to implement reproducible dropout.
	+ test dropout: figure out matlab seed, caffe layer.

2015-02-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	+ juliagpu implementation:
	+ inplace or devectorize macros adapted for gpu.
	x need to solve collections of options to c.
	x need to solve collections of arrays to c.
	+ there should be a generic julia implementation.
	+ the gpu support should be activated from the main script.

	+ speed test?
	+ momentum kernel, shrink code.
	x cpu/gpu blobs like caffe?  main goal: generic readable code.

	+ implement cuda/predict.
	+ implement cuda/backprop.
	+ implement cuda/train.
	+ implement data.h5 comparison.
	+ implement matlab/predict.
	+ compare cuda/predict to matlab.
	+ implement layer.h5 comparison.
	+ implement matlab/backprop.
	+ compare cuda/backprop to matlab. 
	+ implement matlab/train.
	+ compare cuda/train to matlab. 
	+ implement caffe/predict.
	+ implement caffe/backprop.
	+ implement caffe/train.
	+ compare cuda/predict to caffe.
	+ compare cuda/backprop to caffe. 
	+ compare cuda/train to caffe. 

	train options?
	+ already in file?
	+ take as cmd line opts?
	x try all variations?
	+ we'll need cmd-line opts at least for batch, epoch, etc.
	x (or assume epoch=1 and batch=100?)
	x yeah, simple train interface with train x l1 l2 .. y as well.
	x these are just test scripts after all.
	x maybe just do batch in the future.
	+ julia version:
	x layers fully opaque?
	+ train options?
	+ separate options from weights?
