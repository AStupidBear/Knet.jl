2015-10-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	0030 complete gradcheck, predict in addition to train/test for models other than fnn
	0050 saman: local contrast normalization, manual weight bug.
	0100 ai-mtg: Onur and Ozan's bidirectional model.
	0102 yonatan supertagging
	0103 vivi concatenation
	0105 ashish nce model
	0106 karpathy character based models, inputless networks with random generators
	0110 handout-mtg Also conv/pool/rnd/con in the handout have not yet been implemented.
	0110 need train/test switch for nce and dropout.
	0110 src/net.jl: handle con and rnd in tosave.
	0110 src/op/drop.jl: implement rnd.  implement dropout using rnd and mul.
	0120 need something for turkish translation to report to tubitak
	0124 attention s2s model
	0125 ntm
	0126 ctc apply speech model to mt
	0127 image captioning example
	0130 language learning in minecraft
	0160 learning to interpret python ,mfind other examples
	0200 figure out the difference between copyseq and barret.
	0200 barret optimization: for sparse input, keep dw dense, but use a mask to make w += dw faster.
	0200 examples/rnnlm.jl: BUG: start does not handle dense arrays. ArgParse does not handle Real.
	0200 mul2: can probably be implemented using blas sbmv with diagonal m.  (else change order to x1,x2,y)
	0200 optimization: gemm can do incremental updates: pass incr back to ops instead of creating tmp.
	0250 implement gru using con. or axpb?
	0250 src/model/s2s.jl: implement gcheck.
	0300 dynamisarrays add gpu conditionals and cpu only tests, sparse?; should turn constructors into convert.
	0300 package KUdense as DynamicArrays.jl and post it.
	0350 write mikolov ptb downloader like mnist. maybe use github lfs?
	0700 examples/mnist4d.jl (lemlp): TODO: debug softloss carefully, mlp diff has grown.  Debug/implement other losses.
	0700 op/loss.jl: Fix/test all losses.  Update comments del l.y.  No need for forw.  Just loss, which returns loss and sets gradient.  Check tmp use avoid alloc.  Retire/fix logploss, xentloss, percloss, scalloss.
	0700 src/op/compound.jl: finish documentation for all ops.
	0700 src/op/loss.jl: cleanup, move out of op.
	0799 callbacks for update: registering the operation with atreplinit(): : registry look at atexit etc for examples from julia.
	0800 Barrett's training tricks from Google: 1. Element wise clip the cell states so they do not go above 50, 2. Clip the norm elementwise at a threshold value (I do not know what they use), 3. Take the total norm of all matrices except the input embeddings and rescale the norm so it is <=5
	0800 Ozan's optimization tricks: We are using adam. There is no common choice for optimization method. People use adam, rmsprop, momentum or their own learning rate schedule. Some tricks: - Norm clipping - Truncated backprop for large sequences - Batching sequences - Initializing hidden to hidden weights with identity matrix for elman type networks with relu activation - Large initial values for forget gate bias in LSTMs.  In response to: are you guys using adam, rmsprop, adadelta, adagrad etc?  what tricks to people prefer for training rnns?
	0800 implement alternatives to adagrad: adam, rmsprop, adadelta. test on bparser.
	0800 update.jl: add norm clipping and other Google tricks: clip types: calc(per-element, per-matrix, all-recurr, all-model) x clip(g, w, x) x (all, recurr) ?
	0900 Work on non-gpu compatibility.
	1000 add xnorm in addition to wnorm and gnorm
	1000 ai-mtg: Being able to access arrays with original variable names.
	1000 ai-mtg: Copy instruction for going back two time steps.
	1000 ai-mtg: kernels and perceptrons.
	1000 src/model/s2s.jl: implement predict, i.e. decoder.
	1000 consider moving from y=f(x) to f(x,y) for the knet language.
	1000 cusparse.jl: check efficiency of resizecopy for different array types.
	1000 examples/adding.jl: check out unstability of 400. share results with authors.
	1000 examples/mnist4d.jl: test with dropout
	1000 examples/mnistpixels.jl: gcheck gives terrible results, check.
	1000 examples/mnistsparse.jl: gcheck does not work?
	1000 examples/rnnlm.jl: did not get good results on the large experiment, debug.
	1000 examples: consolidate example options in one file.  fix train and predict.  turn libsvm2sparse into a data generator.
	1000 examples: reimplement predict, train, and tutorial.jl.
	1000 find the three parameter element wise mul in julia and rename mul2
	1000 get rid of input() and use constructor arguments instead?	can we?
	1000 gradcheck for sequences does not work: it has to go through a whole sequence, implement as option or separately.
	1000 handout-mtg Replace 0's with ? or _ in dimensions.
	1000 implement maxnorm activation function.
	1000 initforw/forw: write in-place transpose for dy or decide on extra space for ops that need temp space (use tmp?).
	1000 initforw: type/size inference in inputless networks?
	1000 kperceptron.jl: reimplement within the new framework.
	1000 linalg.jl: A_mul_B!{T}(C::CudaMatrix{T}, A::CudaMatrix{T}, B::CudaSparseMatrixCSC{T}) allocates.
	1000 linalg.jl: axpy!{T}(a,x::CudaSparseMatrixCSR{T},y::CudaSparseMatrixCSR{T}) should check sparseness pattern.
	1000 minibatch resizing
	1000 model.jl: add predict, load, save
	1000 nvidia bug check: actf.jl: cudnnSoftmaxBackward off by a factor of 2? came back saying there is no bug.  also report the cusparse transpose problem.
	1000 par and loss should probably get out of op.
	1000 perceptron and structured perceptron examples.
	1000 perclossback: these should be scaled 1/nx, why isn't our gradient check complaining?
	1000 pool.jl: CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING is buggy when padding > 0 or else I have no idea what it is doing.
	1000 pool.jl: function cudnnGetPoolingNdForwardOutputDim(pd::PoolingDescriptor, input::AbstractCudaArray) check if added to the library.
	1000 reimplement averaging.
	1000 src/model/fnn.jl: predict has not been tested
	1000 src/net.jl: TODO: eventually get rid of all similar, similar!, issimilar, dsimilar, dtype, dsimilar! etc. defined in array.jl.
	1000 src/netcomp.jl: convert all asserts to meaningful error messages.
	1000 src/op/add.jl,mul.jl: handle scalar input going forw and back (or implement axpb?, scal?)
	1000 src/op/conv.jl,pool.jl: test 3D-4D-5D convolution and pooling.
	1000 src/op/conv.jl: cpu implementation
	1000 src/op/conv.jl: fix old versions to run comparison test.
	1000 src/op/input.jl: try replacing methods with nothing. we should not be using these?
	1000 test/rnntest.jl: back gives eq for layers 18..8, approxeq for layers 7..1: TODO: investigate why
	1000 testing: add a testnet.jl that constructs random nets and does gradient testing: testlayers could not find the pool bug
	1000 update.jl: figure out how to extend update with callbacks so new optimization tricks can be added.
	1000 use templated code for cuda kernels
	1000 util/cudnn.jl: these changes should go into CUDNN
	5000 compiler improvements: find more register sharing for dif in initback.
	9999 find out why profiling does not work: @time does not mix well with @profile.  valgrind detects memory bugs.
	9999 initback: removing the ability for getdx from a subset of the inputs during back, add later if necessary.
	9999 rename train->train! ??

	* DONE:
	DONE wdot wconv bias can be just single parameter versions of dot, conv, add.  no  need for New names: decided this wasn't a good idea, it is only going to effect dot/add/conv and it is going to create more confusion.
	DONE download/install barrets code for testing.
	DONE compare s2s with barret without gclip
	DONE reimplement mlp/rnnlm with the repeat instruction


2015-10-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	DONE barett s2s test (copying english?)
	DONE examples/copyseq.jl: implement train/test.
	DONE profile s2s
	DONE s2s.jl: add warning if not using the whole dataset.
	DONE src/net/initforw.jl: create and use toforw to avoid unnecessary forw calculation. Eliminate S2C! It turns out you can't. Forw doesn't know if ygold=nothing.
	DONE add copyseq to runtests.

	* copyseq-profile:
	Do not use a batchsize less than 100:
	Powers of two do help

		number of sentences
	batch	10000	5120	6400	sparse
	1025	21.63
	1024	16.33	7.68
	1000	18.10
	512	17.50	7.22
	500	18.24
	256	19.09	7.52	10.41	8.36
	200	20.06	7.53	12.81	8.83
	128	20.44	9.62	10.78	9.17
	100	22.29	10.54	11.85	10.02
	64	25.48	9.87	13.69	11.47
	32	36.50	14.14
	16	57.60

	Further improvements using batchsize=128 corpus=6400 from 9.17
	6.20 iw=csr dw=dense removed decoder output bias looking at copyseq.3.prof
	6.06 if we use dw=ArrayAccumulator
	7.67 if we use iw=dw=dense
	3.82 if we use iw=csru dw=ArrayAccumulator
	4.01 if we turn loss/norm calculation back on
	11.13 iw=dw=dense in comparison
	6.42 iw=csr dw=dense
>>>	3.96 iw=csru dw=dense
	2.55 gpusync=nothing
	2.06 batchsize=256

	iw=csru dw=dense gpusync=nothing whole ptb.train.txt, trying batchsize
	41.01 128 (1,1085.8583984375,6.990126f0,0,0)
	38.90 128 noloss
	31.88 256 noloss
	24.64 512 noloss
	21.82 1024 noloss

	iw=csru dw=dense gpusync=nothing whole ptb.train.txt, batch=128, gclip=0, trying lr
	0.1 (1,1085.8583984375,6.990126f0,0,0)
	0.2 (1,956.5218505859375,6.8633037f0,0,0)
	0.5 (1,958.371826171875,6.865236f0,0,0)
>>>	1.0 (1,674.5174560546875,6.5139976f0,0,0)
	1.5 (1,NaN,NaN32,0,0)

	...lr=1, trying gclip
	inf (1,674.5174560546875,6.5139976f0,0,0)
	100 (1,763.9644165039062,6.638521f0,0,55.73273f0)
	50  (1,757.4146728515625,6.629911f0,0,270.44965f0)
	20  (1,681.4532470703125,6.5242276f0,0,95.472626f0)
>>>	10  (1,633.58251953125,6.4513903f0,0,109.699844f0)
	5   (1,681.5367431640625,6.52435f0,0,128.67555f0)
	2   (1,748.1823120117188,6.6176467f0,0,84.69178f0)
	1   (1,755.4359741210938,6.627295f0,0,101.31935f0)

	...gclip=10, trying lr
	.5  (1,743.931884765625,6.6119494f0,0,101.189766f0)
	1   (1,633.58251953125,6.4513903f0,0,109.699844f0)
	1.5 (1,560.0001831054688,6.327937f0,0,55.12631f0)
	2   (1,541.9075927734375,6.2950954f0,0,33.691235f0)
	2.5 (1,550.0517578125,6.3100123f0,0,55.207405f0)
	3   (1,581.8964233398438,6.3662925f0,0,55.3924f0)
	6   (1,711.7434692382812,6.5677176f0,0,28.626701f0)

	lr=2, final gclip check
	5   (1,585.214111328125,6.371978f0,0,188.39285f0)
	10  (1,541.9075927734375,6.2950954f0,0,33.691235f0)
	20  (1,582.9039306640625,6.3680224f0,0,59.838856f0)

	lr=2, gc=10, impact of batchsize on time and accuracy
	1024 23.05 (1,2182.025390625,7.688009f0,0,104.297005f0)
	512  25.93 (1,1387.432861328125,7.2352104f0,0,79.66049f0)
	256  33.57 (1,887.585693359375,6.788505f0,0,42.632828f0)
	128  41.15 (1,541.9075927734375,6.2950954f0,0,33.691235f0)
	64   64.62 (1,315.03179931640625,5.7526736f0,0,56.093933f0)
	32  112.83 (1,173.69186401367188,5.157283f0,0,91.488815f0)
	16  213.02 (1,116.3692398071289,4.756768f0,0,179.33853f0)

	where do they get in 2 epochs?
	1024 46.06 (2,949.5081787109375,6.855944f0,0,48.838074f0)  < 128
	512  52.42 (2,684.6193237304688,6.528863f0,0,191.49686f0)  < 128
	256  66.53 (2,399.59967041015625,5.9904633f0,0,36.83023f0) < 64
	128  82.07 (2,207.21353149414062,5.33375f0,0,54.024696f0)  ?
	64  129.33 (2,123.24575805664062,4.8141804f0,0,56.24083f0) ?
	32  224.85 (2,67.45431518554688,4.2114506f0,0,135.91908f0) > 16

	and 3 epochs
	1024 69.14 (3,717.0039672851562,6.5750813f0,0,34.406246f0) < 64
	512  79.04 (3,454.5836181640625,6.119382f0,0,34.60331f0)   < 128
	256  99.90 (3,218.51165771484375,5.3868394f0,0,60.949215f0) < 128
	128 123.82 (3,119.21369934082031,4.7809176f0,0,68.25633f0) > 64
	64  194.36 (3,75.53480529785156,4.3245935f0,0,92.08937f0)
	32  333.22 (3,44.343299865722656,3.7919617f0,0,203.678f0)

	* best: lr=2 gclip=10 batch=128 iw=csru dw=dense
	1      46.8345 603.903 353.193 345.621 5.84534 0      69.7023 
	2      89.9217 203.996 229.068 216.359 5.37694 0      52.1384 
	3      133.053 116.824 119.512 117.12 4.7632 0      41.745 
	4      176.464 84.6054 90.0316 87.8274 4.47537 0      77.5709 
	5      220.23 64.2483 67.7716 65.6518 4.18437 0      69.8705

	playing with hidden size, comparing output at epoch=20:
	100  20     869.202 19.9098 32.8415 31.4728 3.44912 0      139.686 
	200  20     1320.63 10.4122 18.7426 18.1979 2.90131 0      140.008 
	512  20     2400.58 4.05138 18.1581 17.5052 2.8625 0      139.964
	1024 20     5028.53 2.59843 24.3922 23.2176 3.14491 0      116.486 

2015-10-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	DONE @knet function add2(x,y;out=0) ... end syntax for ops and models.
	DONE ess bugs
	DONE examples/adding.jl: --nettype lstm does not work? - fixed bug
	DONE src/op/compound.jl: add manual init to wconv and wdot -- using init.

2015-10-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* ess-bug: (DONE) happens after constructors: Net( or Constant( etc.  changing fieldnames->names works but breaks 0.3.  find a solution that will work on both.
	Debugger entered--Lisp error: (wrong-type-argument sequencep names)
	#[(s1 s2) "G	GW\207" [s1 s2] 2](names WARNING:)
	sort((WARNING:) #[(s1 s2) "G	GW\207" [s1 s2] 2])
	ess-julia-eldoc-function()
	#[0 "\302 \204 \205G \303\304!\210\304\207	\203 \303	 !\207\305 \306 \211\204$ \304\202B @=\2038 \307\310\"\206B \311!\202B \311!\206B \307\310\"\303!\266\203\207" [eldoc-last-message eldoc-documentation-function eldoc-display-message-p eldoc-message nil eldoc-current-symbol eldoc-fnsym-in-current-sexp apply eldoc-get-fnsym-args-string eldoc-get-var-docstring] 5 "\n\n(fn)"]()
	funcall(#[0 "\302 \204 \205G \303\304!\210\304\207	\203 \303	 !\207\305 \306 \211\204$ \304\202B @=\2038 \307\310\"\206B \311!\202B \311!\206B \307\310\"\303!\266\203\207" [eldoc-last-message eldoc-documentation-function eldoc-display-message-p eldoc-message nil eldoc-current-symbol eldoc-fnsym-in-current-sexp apply eldoc-get-fnsym-args-string eldoc-get-var-docstring] 5 "\n\n(fn)"])
	eldoc-print-current-symbol-info()
	#[0 "\205 \301 \207" [eldoc-mode eldoc-print-current-symbol-info] 1 "\n\n(fn)"]()
	apply(#[0 "\205 \301 \207" [eldoc-mode eldoc-print-current-symbol-info] 1 "\n\n(fn)"] nil)
	byte-code("r\301\302H\303H\"\210)\301\207" [timer apply 5 6] 4)
	timer-event-handler([t 0 0 100000 t #[0 "\205 \301 \207" [eldoc-mode eldoc-print-current-symbol-info] 1 "\n\n(fn)"] nil idle 0])

	* eldoc: (DONE) after display(:
	Debugger entered--Lisp error: (wrong-type-argument sequencep text/plain)
	#[(s1 s2) "G	GW\207" [s1 s2] 2](")},x)" text/plain)
	sort(("(d::Display,mime::AbstractString,x)") #[(s1 s2) "G	GW\207" [s1 s2] 2])
	ess-julia-eldoc-function()
	#[0 "\302 \204 \205G \303\304!\210\304\207	\203 \303	 !\207\305 \306 \211\204$ \304\202B @=\2038 \307\310\"\206B \311!\202B \311!\206B \307\310\"\303!\266\203\207" [eldoc-last-message eldoc-documentation-function eldoc-display-message-p eldoc-message nil eldoc-current-symbol eldoc-fnsym-in-current-sexp apply eldoc-get-fnsym-args-string eldoc-get-var-docstring] 5 "\n\n(fn)"]()
	funcall(#[0 "\302 \204 \205G \303\304!\210\304\207	\203 \303	 !\207\305 \306 \211\204$ \304\202B @=\2038 \307\310\"\206B \311!\202B \311!\206B \307\310\"\303!\266\203\207" [eldoc-last-message eldoc-documentation-function eldoc-display-message-p eldoc-message nil eldoc-current-symbol eldoc-fnsym-in-current-sexp apply eldoc-get-fnsym-args-string eldoc-get-var-docstring] 5 "\n\n(fn)"])
	eldoc-print-current-symbol-info()
	#[0 "\205 \301 \207" [eldoc-mode eldoc-print-current-symbol-info] 1 "\n\n(fn)"]()
	apply(#[0 "\205 \301 \207" [eldoc-mode eldoc-print-current-symbol-info] 1 "\n\n(fn)"] nil)
	byte-code("r\301\302H\303H\"\210)\301\207" [timer apply 5 6] 4)
	timer-event-handler([t 0 0 100000 t #[0 "\205 \301 \207" [eldoc-mode eldoc-print-current-symbol-info] 1 "\n\n(fn)"] nil idle 0])


2015-10-13  Deniz Yuret  <dyuret@ku.edu.tr>

	* @knet: new interface issues:
	- @knet takes a function definition, ends up defining a function,
	with the given name, but one that returns interpolated
	expressions, not evaluating them.
	- the arguments will have to create input expressions.
	- do we still need the input op?
	- keyword args in the body need careful handling.
	- we have ops and knet functions, any interface difference?
	- knet functions used to define both models and new ops, any
	interface difference?
	- at what point do we use gensyms?

	@knet function logreg(x; out=0)  # compiler can handle x without explicit input stmts; we still need input registers so input ops will be in the compiler output
	    y = wdot(x; out=out)         # careful interpolating keyword args, rhs interpolates, lhs doesn't
	    soft(y)                      # assignment could be optional for last stmt?
	    # soft(wdot(x; out=out))     # support embedding?
	end

	@knet function wdot(x; out=0, winit=Gaussian(0,.01), o...)
	    w = par(out,0; init=winit, o...) # careful interpolating o...
	    y = dot(w,x)
	end

	- compiler actually calls these functions during compilation
	- netstmt separates args and params, params for constructor, args for op
	- if the result is an op, it is inserted into ops,inputs,output
	- if the result is an Expr, it is further compiled by subcomp.

	* DONE:
	DONE turn off gpusync() when done with profiling

2015-10-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE: decided to keep dw dense for now, speed is similar to ArrayAccumulator and no problems with vecnorm.
	DONE cudnn: update doc and release cudnn
	DONE expensive sparse axpy: check if sparsity pattern remains the same, and do elementwise on nzval when you can. examples/mnistsparse.jl; investigate sparsity patterns for increment (not being used in mnist).  examples/mnistsparse.jl: axpy!(::Int64, ::CUSPARSE.CudaSparseMatrixCSR{Float64}, ::CUDArt.CudaArray{Float64,2})
	DONE performance improvements based on rnnlm profile: need better sparse back: A_mul_Bst->sparse and axpy

	* rnnlm-profile: using sparse input and accummulator dw matrix.
	Note that turning on gpusync for profiling slows it down from 16
	secs to 23 secs.

	1.0000	   22855 ...nn/examples/rnnlm.jl; rnnlm; line: 41
	0.3590	    8206  ...net/src/model/rnn.jl; train; line: 13	# forw
	0.3405	     7781 ...Knet/src/net/forw.jl; forw; line: 23
	0.1380	      3155 .../Knet/src/op/add.jl; forw; line: 32	# biasforw
	0.1380	       3154 .../Knet/src/op/add.jl; biasforw; line: 40
	0.1439	      3288 .../Knet/src/op/dot.jl; forw; line: 18	# A_mul_B! only 166 from sparse
	0.5826	    13315 ...net/src/model/rnn.jl; train; line: 21	# back
	0.4278	     9777 ...Knet/src/net/back.jl; back; line: 41
	0.1069	      2443 .../Knet/src/op/add.jl; back; line: 52	# biasback
	0.0851	      1946 .../Knet/src/op/dot.jl; back; line: 22
	0.0811	       1853 .../src/util/linalg.jl; A_mul_Bt!; line: 36	# dense
	0.1410	      3223 .../Knet/src/op/dot.jl; back; line: 23
	0.1401	       3201 .../src/util/linalg.jl; At_mul_B!; line: 37	# dense
	0.0476	     1087 ...Knet/src/net/back.jl; back; line: 51	# axpy! (gpusync)

	*  (ERROR): vecnorm buggy for Acc. use --max_grad_norm 0 --lr 0.1 until fixed.

	* rnnlm-timing: using ptb.test.txt only: dw += iw; w += dw; w always dense.
	15.27 sec: Float32, iw:sparse, dw:dense, kernel mul, regulr axpy, loss=700.67 buggy: kernel mul produces csru, needs atomicAdd
>>>	15.39 sec: Float32, iw:sparse, dw:dense, kernel mul, atomic axpy, loss=688.35
	15.75 sec: Float32, iw:sparse, dw:spacc, kernel mul, atomic axpy, loss=699.17 ??? spacc vecnorm bug
	16.34 sec: Float32, iw:sparse, dw:dense, cusprs mul, regulr axpy, loss=687.96
	16.39 sec: Float32, iw:sparse, dw:dense, cusprs mul, atomic axpy, loss=687.96
	16.88 sec: Float32, iw:sparse, dw:spars, kernel mul, sparse axpy, loss=693.77 ??? axpy needs uniq?
	19.39 sec: Float32, iw:sparse, dw:spars, cusprs mul, sparse axpy, loss=687.67
	19.57 sec: Float32, iw:dense,  dw:dense, dense  mul, dense  axpy, loss=686.37622
	(ep,perp...,wmax,gmax,lr) = (1,686.376220703125,256.7232f0,153.58725f0,1.0) # dense reference

	19.47 sec: Float64, iw:sparse, dw:dense, kernel mul, regulr axpy, loss=704.61 buggy
	19.57 sec: Float64, iw:sparse, dw:dense, kernel mul, atomic axpy, loss=694.39
	20.08 sec: Float64, iw:sparse, dw:spacc, kernel mul, atomic axpy, loss=700.01 ???
	20.86 sec: Float64, iw:sparse, dw:dense, cusprs mul, regulr axpy, loss=694.28
	20.67 sec: Float64, iw:sparse, dw:dense, cusprs mul, atomic axpy, loss=694.39
	20.65 sec: Float64, iw:sparse, dw:spars, kernel mul, sparse axpy, loss=700.01 ??? axpy needs uniq?
	24.33 sec: Float64, iw:sparse, dw:spars, cusprs mul, sparse axpy, loss=694.42
	28.14 sec: Float64, iw:dense,  dw:dense, dense  mul, dense  axpy, loss=694.351734
	(ep,perp...,wmax,gmax,lr) = (1,694.3517340040148,256.38582340534157,159.3564629264121,1.0) # dense reference

2015-10-10  Deniz Yuret  <dyuret@ku.edu.tr>


	* DONE:
	DONE do we need 2 forw?  1 forw may make the predict interface easier. we could have a keepstate like option for sequences.
	DONE ai-mtg: We need a predict function: new forw can return out, back return dx for net stitching?
	DONE op/loss.jl: Take loss out of the model, make it a train/test option.
	DONE examples/adding.jl: figure out why the new data generator did not work.
	DONE allow tuples of any length for data, (y,) for unsupervised/inputless, (x1,x2,y) for multi-input
	DONE examples/rnnlm.jl: loss per sequence or loss per token?
	DONE examples/s2c.jl: find better way to stitch networks together without exposing guts.
	DONE embedded expr: do we really need to?  abstraction takes care of most variable hiding.
	DONE handout-mtg Avoid variables by allowing embedded expressions: we created compound instead.
	DONE model.jl: handle the case where the data has tuples of length 1 or more than 2.
	DONE src/net/forw.jl: get rid of sequence forw? have one type of forw with options (seq vs keepstate do we need both?)
	DONE src/net/forw.jl: make yout optional last argument to match the op interface? returning internal y instead.
	DONE take loss out: that way there is no need for accuracy vs test difference.  deprecate accuracy.  consider sequence vs item loss.  implement zeroone, perceptron, etc.
	DONE need better net combo: created model/* instead.

	* examples/mnistpixels.jl: How does s2c work in the new interface?
	[1] The sequence proceeds with x=pixel, y=nothing.  At the end we
	could have x=nothing, y=class.  That could be the sign to run
	net2.  [2] Conceptually simpler might be to always have net1/net2
	in a stack.  Maybe the compiler figures out net2 is not necessary
	to run as long as y=nothing.  With the last pixel we would have
	y=class.  Either that could trigger backprop or an extra
	item=nothing after the last pixel.  How much of this is
	generalizable to s2s?  There we definitely need item=nothing to
	signal end of sequence.  We could imagine stacking net1 and net2
	there as well?  No, because during decoding input is directly fed
	into net2.  So we will have to have different train routines?
	That ruins our original train(model,data) interface.  Unless we go
	back to data elements possibly being sequences, is there any way
	to salvage that?  It seems we are moving complexity from forw/back
	into train/test.  New itembased picture.  Models only input/output
	individual items, not sequences.  Data only generates individual
	items, not sequences.  Then if we are doing different things with
	different items, train/test has to deal with it?  I should look at
	NTM and the attention models now so I don't regret design
	decisions later.  But right now it looks like model.jl is
	deprecated, each model type will have to have its own
	data/train/test.  There is not common interface.

2015-10-09  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	DONE initback: minimize array zeroing.
	DONE src/op/conv.jl: implement xavier in par.jl.
	DONE src/op2: start a collection of compound ops: wdot, bias, lstm, wbf...

	* forw-loss-back: What is the best interface?
	forw(m,x) => y
	loss(m,ygold) => ygrad
	back(m,ygrad) => xgrad

	There is no point in making loss and back functional by providing
	the output y.  The output y along with a number of internal
	outputs have to be remembered by the network.  Even if we made it
	functional, giving back a different y would not work because the
	internal outputs would be not consistent with it.

	Pre-allocating arrays outside (y for forw, ygrad for loss, xgrad
	for back) results in unnecessary copying.  Instead we resign to
	the fact that the model shares internal arrays and copy them if
	necessary.  What about loss vs grad calculation?  We can use two
	functions:

	loss(m,ygold) => loss value
	grad(m,ygold) => ygrad array

	Or use a single function that has lots of options.  We need to be
	able to choose from a number of loss functions.  These could be
	keyword arguments, or we could have quadloss(m,ygold),
	softloss(m,ygold) etc.  The sequence of events is:

	train: x->forw->y, ygold->grad->ygrad, ygrad->back->xgrad.

	No need for loss value unless asked for.  y, ygrad, xgrad are
	internally allocated, x and ygold come from outside.  We are
	losing the ability to state which xgrad we want from
	back.

	test: x->forw->y, ygold->loss->lossval
	predict: x->forw->y, there is no ygold

	hcat: x->forw1->y1, y1->forw2->y2 (no need to alloc/copy y1 if it
	is already on gpu).  ygold2->grad2->ygrad2, ygrad2->back2->ygrad1,
	ygrad1->back1->xgrad1.  (again should not have two copies of
	ygrad1).

	Ability to use different loss functions:

	test: x->forw->y, ygold->loss->lossval (loss should be an arg to test).
	train: x->forw->y, ygold->grad->ygrad (grad should be an arg to train).

	Low level loss fn is defined as loss(y,ygold)->lossval
	Low level grad fn is defined as grad(y,ygold,ygrad)->ygrad

	Defining a single loss function signature:
	loss(y,ygold) -> computes loss
	loss(y,ygold,ygrad) -> computes loss and grad
	loss(y,ygold,ygrad; getloss=false) -> computes grad

	Is there any advantage of merging loss and grad together.  For
	softloss the calculations are independent:
	loss = -sum(ygold * log(y))
	grad = 1 - ygold/y

	quadloss not so; grad is necessary for loss.
	grad = y-ygold
	loss = (1/2)*sum((y-ygold)^2)

	xentloss: both need ynorm at some point
	loss = -sum(ygold * log(ynorm)) where ynorm is normalized exp(y)/z
	grad = ynorm - ygold

	Mainly we want to be able to:
	test(model,data; loss=quadloss)
	test(model,data; loss=percloss)
	test(model,data; loss=xentloss)
	train(model,data; loss=softloss)
	train(model,data; loss=quadloss)

	We don't want to have to compute lossval unless asked for.

	function train(model,data; lossfn=quadloss)
	    for (x,ygold) in data
	        y = forw(model,x)
	        l = loss(model,ygold; lossfn=lossfn) # optional
	        ygrad = grad(model,ygold; lossfn=lossfn)
	        xgrad = back(model,ygrad)
	        update!(model)
	    end
	end

	Do we need to pass around ygrad?  grad(model,ygold) sets dif[N].
	Just like forw sets out[N].  We don't pass out[N] to back, why
	should we pass dif[N]?

	Or should we do back(model,ygold; lossfn=lossfn)?  That would make
	more sense and mirror forw.  No need for extra grad fn.  This is
	getting close to having our own loss layer.  But loss layer wasn't
	doing anything in forw pass anyway.  And it was doing something
	unusual in the back pass (not treating the input as gradient).  So
	treating it as a special fn is ok.

	Two questions: (1) the xgrad problem and (2) lossval problem.
	lossval (2.1) needs extra space and (2.2) sometimes shares
	computation with grad.  lossval needs y (on gpu) and ygold (on
	cpu).  We could choose to do it on cpu like we used to.  In any
	case the two arg version of lossfn should take care of this.  If
	we do cpu we won't need temp space, if we do gpu we will.  We
	should look at relative speed etc.  (3) what does back call for
	initial gradient?  the three argument version of loss?  (4) how
	does train get its lossval?  through separate call? or getloss
	option to back? (1) how do we tell back we want xgrad?  getgrad
	option? (TODO)  effects the value of toback.  instead of passing
	arrays, pass booleans for dx.

	xloss(y,ygold) -> lossval
	xloss(y,ygold,ygrad) -> overwrites ygrad

	train, test, and back take loss keyword arg.

	(5) sequence vs item training: train needs to know how often to
	run back.  every iteration?  wait until end of sequence
	(item==nothing)?  every n iterations?  split these into separate
	training functions?  how much code in common?  how about
	keepstate?

	seq flag: cannot get rid of it in train (when to run back), and in
	set_toincr (parameters only need incremental updating for seq
	input).  So might as well make it a precondition for push/pop.

2015-10-08  Deniz Yuret  <dyuret@ku.edu.tr>

	* rnnlm-profile:

	msec	ratio	where
	5515	1.0000	model.train
	1620	0.2937	 forw.sforw:48
	10	0.0018	  forw.sforw.initforw:49
	1610	0.2919	  forw.sforw.forw:55
	205	0.0372	    forw.push:20
	800	0.1451	    forw.opforw:25
	119	0.0216	     actf
	7	0.0013	     softmax
	47	0.0085	     add
	127	0.0230	     bias
	243	0.0441	     dot
	117	0.0212	      dense.dot.dense
	102	0.0185	      dense.dot.sparse (y=w*xS)
	11	0.0020	     loss.copy
	24	0.0044	     mul
	13	0.0024	     par.init
	512	0.0928	    forw.oploss:32
	66	0.0120	     soft.similar (fix tmp?)
	31	0.0056	     soft.softloss64csc
>>>	403	0.0731	     soft.asum (rewrite asum: did not speed up)
	3808	0.6905	 train.back:49
	46	0.0083	  back.initback:55
	3762	0.6821	  back.back:58
	2606	0.4725	   back.opback:30
	75	0.0136	    sigmback
	41	0.0074	    softback
	78	0.0141	    tanhback
	116	0.0210	    biasback
	1909	0.3461	    dot.A_mul_Bt
	154	0.0279	     dense.dense->dense
>>>	1256	0.2277	     dense.sparse->sparse:79+3 (sparsify?, write kernel? /home/nlg-05/dy_052/julia/latest/base/sparse/linalg.jl:110)
	457	0.0829	     dense.sparse->sparse:80 (gemm!)
	123	0.0223	    dot.At_mul_B
	1007	0.1826	   back.axpy:33
	183	0.0332	    axpy.dense
>>>	562	0.1019	    axpy.sparse.geam:89 (could just add nzVal?)
	251	0.0455	    axpy.sparse.free:90
	30	0.0054	 train.wnorm:51
	28	0.0051	 train.gnorm:52
	26	0.0047	 train.update!:53

	* linreg-profile:
	1. before the vecnorm optimization
	2. after the vecnorm optimization

	ratio1	msec1	ratio2	msec2
	1.0000	1676	1.0000	1303	train
	0.0525	88	0.0660	86	train.next
	0.0961	161	0.1105	144	train.gradcheck
	0.2942	493	0.3231	421	train.forw
	0.0066	11	0.0138	18	train.forw.initforw
	0.0066	11	0.0069	9	train.forw.x
	0.0292	49	0.0284	37	train.forw.copy
	0.0525	88	0.0698	91	train.forw.forw
	0.0280	47	0.0261	34	train.forw.forw.dot
	0.0227	38	0.0215	28	train.forw.forw.dot.gemm!
	0.0119	20	0.0246	32	train.forw.forw.loss
	0.0119	20	0.0230	30	train.forw.forw.loss.copy! ??? why do we copy here? will change when we take losslayer out of model.
	0.1957	328	0.1865	243	train.forw.quadlossloss
	0.0209	35	0.0545	71	train.forw.quadlossloss.similar
	0.0131	22	0.0246	32	train.forw.quadlossloss.copy!
	0.0125	21	0.0200	26	train.forw.quadlossloss.axpy!
>>	0.1492	250	0.0844	110	train.forw.quadlossloss.vecnorm
	0.1390	233	0.2571	335	train.back
	0.0507	85	0.1167	152	train.back.initback
	0.0442	74	0.0944	123	train.back.initback.fill!.dif0
	0.0107	18	0.0161	21	train.back.copy!.dif0.dy
	0.0686	115	0.0975	127	train.back.back
	0.0215	36	0.0361	47	train.back.back.dot
	0.0322	54	0.0384	50	train.back.back.quadloss
>>	0.1897	318	0.0913	119	train.wnorm
>>	0.1981	332	0.0867	113	train.gnorm
	0.0298	50	0.0637	83	train.update!

2015-10-02  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	+ fix pdf
	+ setup test harness.
	+ make conv accept all cudnn options.
	DONE handout-mtg: initforw/initback: The array sharing optimizations are currently turned off.
	DONE deprecate/fix KUdense?  edit colops and linalg to fit.
	DONE put optimizations back.
	DONE profile rnnlm and check out the number of registers in lstm: come up with steps and time spent on each step for various models.


2015-09-28  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE: fix conv/pool. solve compiler namespace problem.

2015-09-27  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/KUnet.jl: DONE: figure out the namespace issue.  compiler
	does not work when inside the module.

	* examples/adding.jl: DONE: make sure back does not call ops with
	nothing.

	* examples/mnist2d.jl (net): DONE: find out where the slight
	difference comes from. bias=0.

	* src/netcomp.jl: for special par ops do we have valid flags?
	tosave: false, never necessary since we do not change between iterations
	toincr: always true if seq, true when multi otherwise
	toback: always true
	tozero: not necessary for anyone with nothings

	for regular ops during non-seq runs:
	tosave: always false, no need for stack
	toincr: multi should be true
	toback: true if par descendent. cycles? handled.
	tozero: deprecated

	following should work:
	tosave: !par & seq & back_needs_it: netcomp sets !par &
	        back_needs_it, use seq as extra condition for push/pop. ok.
	toincr: multi || (par & seq): netcomp gives multi,
	        we need to handle par & seq
	toback: par or descendent (fixed): netcomp sets it.

	* DONE: toincr is a problem:
	- netinit:33 difsparse says no if toincr, why?
	- paramaters are incrementally updated in a sequence.
	- however we want to use sparse dw to go with sparse x.
	- we have to do this (hoping sparsity pattern stays the same)
	- finddif also uses it to not pick multi ops for sharing (also
	excludes par).
	- netback:9  toincr[N] is it possible for output to be a par?
	- this also effects which tmp are created.
	- should we take this out of net and make it a keyword arg?
	- we can forbit seq to switch from call to call.
	  just like we dont allow sparsity or element types change.
	- seq vs no seq effects tosave/stack, toincr, thus tmp.

	* src/op/pool.jl: DONE: rewrite.

	* src/op/conv.jl: DONE: rewrite.

	* src/netback.jl:
	# DONE: use dx=nothing instead of returndx=false, that way we can choose which dx to return for multi-input case.
	# DONE: use path analysis to stop back/dx calculation for any path that does not lead to a parameter.

	* docs/rnn.md (example): DONE:
	## Interface issues:

	op interface:
	forw(op, x..., y)
	back(op, dy, dx...; x, y)

	net interface:
	forw(net, x...; yout, ygold)
	back(net, dy; dx)

	* src/netinit.jl: I should test with unoptimized version where all
	arrays are distinct and all are zeroed at the beginning, slowly
	introducing optimizations.

	* DONE:
	./KUnet.jl
	./data.jl
	./model.jl
	./model/*: update -> examples
	./model/irnn.jl: update -> examples
	./model/kperceptron.jl: replace
	./model/lstm.jl: update -> examples
	./model/s2c.jl: update -> examples
	./netback.jl: REWRITE
	./netcomp.jl: done
	./netforw.jl: done
	./netinit.jl: complete back
	./nettest.jl: deprecate
	./netutil.jl: done
	./op.jl: done
	./op/actf.jl: test
	./op/add.jl: test
	./op/conv.jl: update
	./op/dot.jl: test
	./op/drop.jl: replace
	./op/input.jl: done
	./op/loss.jl: test
	./op/mul.jl: test
	./op/par.jl: opts, averaging, adagrad etc.
	./op/pool.jl: update
	./update.jl: REWRITE -> par?
	./util/*: check usage
	./util/array.jl: check usage
	./util/colops.jl: check usage
	./util/cudart.jl
	./util/curand.jl
	./util/cusparse.jl
	./util/deepcopy.jl
	./util/dense.jl: deprecate?
	./util/gpu.jl
	./util/linalg.jl

2015-09-26  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/netforw.jl:
	# DONE
	# + performance: figure out when no back needed, no returndx needed
	# + performance: do better register optimization
	# + if an op is using an external array, it should not store it.
	# + if we take xi/yi as a parameter for back maybe the net would not have to remember it?
	# x rename train->trn for ops: decided mode=:train
	# x rename compiler->net, net->netfunc?
	# + change calling convention to forw(y,x...)
	# + figure out par resizing.

2015-09-25  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/net.jl: netforw.jl, netback.jl, netinit.jl/netcomp.jl/net.jl?
	(psize): work on size/type inference for both initforw and the compiler.

	* nothing: DONE: nothings are a pain.  we use them to prevent
	unnecessary allocation and copying of zero arrays.  there has to
	be a better way.  maybe constrain them only inside net and don't
	have ops deal with them?
	- nothings are back.  without them there is too many unnecessary zeroing, copying, pushing, popping etc.

	* src/op/par.jl: OK, this is too much too fast, things will be
	broken for too long.  Better to transition gradually.  Use
	existing calling conventions, existing ops.  First make things up
	and running with the new compiler, then think of consolidating the
	code.

2015-09-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* examples/zaremba14.jl: @@hpc
	13311883.hpc-pbs.hpcc.  dy_052      isi      rnnlm1            52076     1      1    --   24:00:00 R  00:09:11   hpc3004/0
	13311993.hpc-pbs.hpcc.  dy_052      isi      zaremba1              0     1      1    --   24:00:00 R  00:05:53   hpc3005/0
	13311994.hpc-pbs.hpcc.  dy_052      isi      zaremba2              0     1      1    --   48:00:00 R  00:05:53   hpc3007/0
	13312198.hpc-pbs.hpcc.  dy_052      isi      rnnlm2              --      1      1    --   48:00:00 Q       --     --


2015-09-23  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE: big plan
	- book: find markdown viewer, code pretty printer, latex code formatter?
	- rnnlm
	- new compiler
	- profiler
	- debugging
	- code todo

	* docs/rnn.md (rnd): DONE: Profile and stop useless dx calc

2015-09-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/op/mmul.jl: DONE: adding.jl complains about issparse(void), why is
	x void?

2015-09-21  Deniz Yuret  <dyuret@ku.edu.tr>

2015-09-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* examples/mnistsparse.jl: NO: fix sparse arrays.  Rename KUdense
	-> DynamicArrayCPU/GPU.  SparseArrayCPU/GPU.  Decided to retire
	them instead.

2015-09-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/testmnist.jl: collect data and code in one place.

	* test/cusparse.jl:

	# nh = 1000
	# nd = 100000
	# nb = 100
	1. GPU forw using csrmm2!(x',w,y')+transpose 0.05ms
	2. GPU back (all sparse) using gemm!(x,dy',dw') 2.13ms
	3. GPU back (all sparse) using gemm!(dy,x',dw) 7.26ms

	For the first we copy sparse minibatch x to x' in gpu,
	which is a direct copy since gpu is csr.  w already right shape.
	Need to transpose y before going further, needs extra space. (THINK)
	There is really no alternative to #1 because sparse x has to come first.

	For the second we get regular dy which we need to transpose, same
	problem as the first.  Now we also need to transpose x and dw.
	Maybe I can get away with dw by setting a transpose bit since I
	have to write w+=dw kernel anyway.  (Cancelled: check update and
	gradient clip).  Transposing x is a pain (THINK).

	The third one is slower but we already have dy, x', and dw comes
	out right.  In fact with other shapes (nh=1500, nd=10000, nb=20)
	third one is faster, so we go with it.

	DONE:
	- write w+=dw kernel for update.
	- update ops should be compatible with sparse dw.
	- get rid of KUsparse, all code uses CUSPARSE.
	- fix loss to deal with sparse dw.
	- assume user provides all arrays in cpu.
	- test sparse mnist.
	- complete lm.
	- write new assembler.

2015-09-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	- new assembly language: (1,Mmul(50),2), (3,Mul2(),4,5)

	subroutine convention: takes input from 0,-1 etc. leaves output at
	last register.  no stack.  could allow symbols, variables? no
	need.  consecutive numbers enforced? makes them redundant.
	allowing symbols makes sense: (:a,Mmul(50),:b,:c).  But then what
	are the inputs?  We could use the Julia parser to get fancy
	:(a=Mmul(50,b)).  Who needs 26 variable names (lstm), numbers are
	just fine for now.  We could have (1,Input()), (2,Input()) to
	prevent arbitrary 0,-1 etc. numbering.  Good.  Eliminate the need
	for empty parens, i.e. (3,Mul2,4,5).  Detect type and create
	object.  Good.  Ops no longer manage input output storage, why not
	take params out as well: (1,Input), (2,Param), (3,Mmul,1,2).  We
	already have a Param type!  Only mmul,bias,conv currently use
	params.  They specify sizes differently, Bias(), Mmul(50),
	Conv(5,20).  Param can init an empty array using same args: Bias
	creates (0,)->(n,), Mmul creates (m,0)->(m,n), Conv creates
	:(x,0,o)->(x,x,i,o).  We can totally do the same!  Param creates
	empty array by adding one more zero dimension than supplied
	:(modify the constructor).  No actually compiler should take care
	of this, just like it adds an evaluation to Mul2.  But that's only
	possible if we use quoted expressions.  Explicitly marking params is cool.
	Increases the number of lines (but there is not bias without
	param!). Ops have no internal memory is cool.  (x's and y's are in
	registers).  (dx's are in registers).  Difference between x's and
	w's?  x's are ephemeral, w's stay.  The Param's are never used by
	anything but the associated op.  Unless we have Param sharing!?
	e.g. each word in the window (represented by one-hot) gets
	multiplied by the same embedding matrix!  ok.  sold.  Nothing
	except Param needs size args, most ops can be written without
	parens, except when we need keyword arguments.  but those argument
	are always passed to param!  Param lines do not create registers,
	other lines do (or we can call them param registers that are more
	permanent and have associated update options).  Or do create
	registers for them, just don't zero them out!  No need for diff,
	inc, they use the same mechanism as other registers.  Same multi
	detection etc?  Or rather keep them separate as (1) they are not
	zeroed in initforw, (2) they are not zeroed (dif) for incremental
	every back step.  But that's fine.  We'll just have conditions.
	So Input and Param will have to be ops to preserve the op/register
	alignment.  How to keep train options? (TODO: change setparam to
	setoption and call them options).  Have a separate option array
	for them?  We are distributing related info in different locations
	rather than one param object.  setoption will have to be provided
	an optional number? (need to remember the original numbers or keep
	them consecutive).  (but there is number xform when we compile
	high level units).  setoption cannot be specified per parameter
	after construction.  global setoption is still possible.  that's
	fine.  what about splitting dropout?  need rand layer. need to
	figure out trn/tst difference.  then just mul2.  Example prog:
	a=:[(1,Input),(2,Param(50)),(3,Mmul,1,2),(4,Param),(5,Bias,3,4),(6,Relu,5)]
	Composition? will change all the numbers.  so we could in
	principle not make param an op.  input is an op that produces
	something.  most likely executed by the net.  but param does not
	produce anything at least anything different every iteration.
	should it crowd the op list even if it is a noop?  Alternative is
	to push them to the end of the list like we do now?  Keep a
	separate set of param registers?  and difparam registers?  naah.
	Param is an op that outputs a constant.  With Input and Rand these
	ops produce an output without reading other registers.
	ok next: compiling from expressions is good, but what about
	composition?  Do we want LSTM to be a symbol as well?  No, LSTM is
	another expression.  That can be embedded: (3,LSTM,5) etc.  Cannot
	be, LSTM has to be a function that takes a size
	parameter. (3,LSTM(10),5).  Could be a constructor.  Compiler will
	have to evaluate and get an expression.  Then can be merged.
	foo(h)=:[(1,Param($h))] works.  If it is not an op, it should be a
	function that returns a net expression.  Nets cannot be composed,
	expressions can be composed.  But what if the LSTM expression have
	embedded expressions.  We got that compilation for free if LSTM
	was a net.  If numbers don't have to be consecutive, merging is
	easy, just renumber, match inputs and outputs.
merge mul2 and drop -> mul
	merge add2 and bias -> add
	mmul -> gemm
	conv, pool
	actf, loss
	define higher levels out of these
	sigm(n)= par(n) gemm par() add sigm
	sigm2(n) = par(n) gemm par(n) gemm add par() add sigm
	drop(p) = rand(p) mul
	can we define gru?
	(1-z) h1 + z h2
	either represent 1-z as a separate op axpy?
	or have an option in mul?
	axpy is par mul par add, except pars are scalar and constant
	(lr=0)
	we need mul and add to be very smart broadcasters.
	and par have some smart init options (ndims?)
	par(5,0) par(0) par(5,5,0,10) always use 0 for unk dimension.
	this way a scalar is par(1). and conv for difft dims is possible.
	add2 : par(0,0), bias: par(0), mul2: par(0,0), rnd(0,0) vs rnd(0)
	no implicit zeros!  dims always clear.
	how do we init par to a known value?  give an array? par([1]).
	par(a) where a is a variable?  compiler passes to eval?
	averaging, adagrad, dropout difft trn tst behavior.  test=false
	option?  test=average option?  all par/rnd options.
	Consider allowing both integers and variables.


	- sparse operations: lm needs:
	: efficient forw w:dense x:sparse => y:dense
	- compare my implementation against cusparse: cusparse a lot faster.
	- cusparse produces transposed output: need temp space to transpose.
	- use something like dif1?
	- for in-place transpose:
	http://dl.acm.org/citation.cfm?id=2555253
	https://github.com/BryanCatanzaro/inplace: still needs some space,
	probably not worth it.
	: efficient loss y:dense + dy:sparse => dx:dense: write your own
	: efficient back dy:dense x':sparse => dw:sparse?
	: efficient update w:dense + dw:sparse =>  w:dense: write your own, same as loss.

	- test cusparse again.
	- update to cudnn v3.

	* src/op/loss.jl: DONE: accept sparse target.

	* src/param.jl: DONE: rename to Param. actually wait for the new assembler, probably end up with par.

2015-08-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* FIXED: /home/nlg-02/data07/eng/google-ngram/CompoundWords.dbg:
	julia my_train_and_random_test.3.jl train.data.part test.data.1
	gives illegal memory access error.  This was an issue with 32 vs
	64 bit integer indices with sparse matrices.

2015-08-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* RNN:
	- an rnn is still a vector of micro-layers.
	- we need two new layers, adder and elementwise multiplier for lstm?
	- each layer (except the adder) has a single input
	- the input is specified by two indices:
	-- index into rnn (absolute?), index into time (relative)
	- the adder can have a list of indices. (should we allow all to
	have this?)
	- x, y become a vector of matrices rather than a single matrix
	- we may want to use different variables for backward
	compatibility: xlist ylist?
	- dx, dy?
	- resetting the vectors?
	- feeding the input?
	- backprop?

2015-08-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE: impement RNNs.

	* DONE: doc before release:
	- Perceptron.
	- Layers.
	- Update (including averaging etc.)
	- ScalLoss?

2015-08-02  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/bias.jl: DONE: Create a @gpu macro.

2015-07-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/kperceptron.jl:
	- klinear != perceptron, averaging difference?
	+ klinear,single,sparse,cpu gets stuck: changed slow algorithm.

	* src/util/sparse.jl: Frequent problem I have been having is due
	to the following: when a function parameter has a parametrized
	type like KUsparse{Array}, that is parametrized by another type,
	it is better to make the inner type a variable.  i.e. instead of
	foo(a::KUsparse{Array}) use foo{A<:Array}(a::KUsparse{A})

2015-07-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/util/linalg.cu (_A_mul_Bs_32): DONE: Test this.

	* src/mmul.jl: DONE:
	+ not using averaging for prediction.
	+ cudaarray is not giving the same answer as array
	+ sparse does not work

	* src/net.jl: conversions in train and predict: do we end up with
	the same array type we started with?

2015-07-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/tutorial.jl: profile, did it get slower?

	* DONE:
	+ test mmul with dense and sparse
	+ fix kperceptron, perceptron
	+ make layers generic, support regular arrays, if possible
	in-place sparse arrays.
	+ unit test cadd!

	* src/util/linalg.jl: DONE:
	+ add linalg tests to dense and sparse.

2015-07-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/testsparse.jl (density): DONE:
	+ implement full
	+ type cannot be constructed: use ::Type{T} notation.
	+ implement uniq! for KUsparse

	* test/testdense.jl: DONE: add colops and linalg? tests.  Write
	testsparse.

2015-07-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* BUGS:
	+ cpucopy/gpucopy: fixed
	- load/save: doubles the storage.
	- savenet is failing.
	- reshape is missing.
	- cpucopy leaves KUparam{CudaArray}
	- cpucopy does not work with net
	- savenet causes seg fault
	- test for cpu only machine
	- introduce update_list and skip_list
	+ turn all cu params into double

	* layers: DONE: Layers are functions.  They should be generic.
	Their output types should match their input types.  Internally
	they can do whatever they want.  If the input type does not match
	the parameters the parameters should be moved?  No, better if only
	flexible at init.  We should support regular arrays not just
	KUarray's for input/output.  KUarray's just make certain
	operations faster, so train/predict can use them during
	minibatching.  We should specify the array types supported by each
	layer.

2015-07-16  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/dense.jl: ok, we have three types of arrays: sparse, dense,
	param.  These have different members, so they can't just be
	parametrizations of a common type.  However each of these has
	array type, element type and dimensions as parameters.

	* src/bias.jl: should rename param for consistency.  data->arr.
	Define GPUparam and CPUparam?

	* src/mmul.jl: where do we need extensible arrays?  x, y,
	dataflow.  Train batches.  Not w params.  Except kernel perceptron
	support vectors and weights.  If we used regular arrays for
	params, load/save problem would go away.  Do we replace each l.x
	with l.x.arr?  Or do we define ops that work on KUarrays?  Then we
	need to consider mixtures.  Best to just work with your own arrays...

	OK, best of both worlds: we have two types of arrays, Param and
	Data arrays.  They have different requirements.  Just define the
	necessary functions for each.  Data arrays are resizeable whereas
	Param arrays have derivatives and associated update parameters.

2015-07-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* ARRAYS:
	We want to support cpu/gpu, sparse/dense, float32/64, 2D/4D dynamic
	(efficiently extendible) array types in KUnet.  The relevant part of
	the current array hierarchy in Julia is as follows:

	AbstractArray
	  DenseArray
	    Array
	  AbstractSparseArray
	    SparseMatrixCSC
	  SubArray

	The CUDArt hierarchy is disconnected:

	AbstractCudaArray
	  CudaArray
	  CudaPitchedArray
	AbstractArray
	  HostArray

	CUDArt also defines two convenience types:

	typealias CdArray{T} Union(DenseArray{T},HostArray{T},AbstractCudaArray{T})
	typealias ContiguousArray{T} Union(Array{T},HostArray{T},CudaArray{T})

	Notes:

	So far I have added CudaDynArray and CudaSparseMatrixCSC to this
	list.

	CPU Arrays currently do not handle resize and hcat efficiently.
	They probably need a special type as well.  We might as well
	define all 4 types.

	Sparse arrays are needed only as inputs and possibly support vector
	matrices.  Things quickly turn dense after that.

	-- Layers Should Adapt to Their Inputs --
	Declaring a model with specific input output array types, vs
	declaring an abstract model which then takes the appropriate shape
	when first batch of data is seen.  Certainly the abstract
	declaration has an ease of use advantage, the constructions are
	more readable.  The more abstract the model code the better.
	However this means a lot of low level initialization code goes
	into layer definitions (initforw etc.).  What if input type
	changes during the lifetime of the model?  We adapt to size
	changes, why not type changes? eltype changes?  Device, sparsity,
	array implementation (there could be more than one sparse array
	type), eltype, ndims: layers should adapt to their input and be
	able to change (maybe with a warning) midstream.  The output types
	(both y and dx) should match the input types (except y may not
	match x in ndims.)

	Operations to be supported:
	train: shufflexy!, size, ndims, x2b
	predict: b2y
	layers: similar! (for changing type), resize (for changing size)
	kperceptron: hcat! (bparse also needs this)
	mmul: A_mul_B! A_mul_Bt!

	Each layer should document which array types supported.

	* DONE: organize code around operations (array.jl, linalg.jl) or
	data types (sparse, cusparse)?

	* DONE: release perceptron.
	+ do a pre-release
	- finish documentation.
	- implement structured perceptron? just a difft training rule?
	- test mmul+percloss, need averaging, implement ASGD.

2015-07-09  Deniz Yuret  <dyuret@ku.edu.tr>

	* qsub: automatic spawn of worker machines in qsub.  From JonMay:
	Once a job starts you will have the environment variable
	PBS_NODEFILE and this will contain a list of machines, one per
	line.

	e.g. (sorry, it happened to be a one-node job and i didn’t want to
	wait for my 2-node demo to start)

	[jonmay@hpc1457 quake]$ echo $PBS_NODEFILE
	/var/spool/torque/aux//12554804.hpc-pbs.hpcc.usc.edu
	[jonmay@hpc1457 quake]$ cat
	/var/spool/torque/aux//12554804.hpc-pbs.hpcc.usc.edu
	hpc1457

2015-07-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/util.jl: DONE: use size! instead of realloc in similar!
	look at size! vs resize! again
	make sure regular arrays will work
	similar! does not match array type?  reconsider its semantics.

	* net.jl: profiling kunet.  nothing much to do.

	# train: most time spent copying data to gpu
	gpu() && gc()  # L46: 76
	for b = 1:batch:ninst # L47: 1
        e = min(ninst, b + batch - 1)
        xx = x2b(xx, x, b:e) # L49: 25641
        yy = x2b(yy, y, b:e) # L50: 39
        backprop(net, xx, yy; o...) # L51: 516
        update(net; o...) # L52: 160
        (iters > 0) && (e/batch >= iters) && break
        gpu() && (gpumem() < (1<<30)) && gc() # L54: 32
	end
	strip!(net)
	gpu() && gc()

	# predict: most time spent on copying back from gpu.
	ninst = size(x, ndims(x))
	(batch == 0 || batch > ninst) && (batch = ninst)
	xx = yy = nothing
	for b = 1:batch:ninst
        e  = min(ninst, b + batch - 1)
        xx = x2b(xx, x, b:e)  # L68: 382
        yy = forw(net, xx; predict=true, o...) # L69: 150
        y  = b2y(y, yy, b:e, x) # L70: 4150
	end
	return y


2015-07-03  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/simpleIPC/fooipc.cu (main): seems like the only choices are
	using lightweight threads or memcopy.  a memory handle seems
	sharable by only one other process?

	* src/cumatrix.jl: DONE: is it better to split code by function
	rather than datatype?  more similarity.

	* src/sparse.jl: DONE: better interface for hcat!, last two args
	should be optional.

	* test/runtests.jl: DONE: test different cpu/gpu sparse/dense
	float32/float64 combos.

	* src/util.jl: DONE: use CudaDynArray everywhere and minimize the realloc.

	* conv.jl: DONE: conv fails tutorial.jl code.  fix
	initialization: should figure out input size/type from first x
	like bias/mmul.

	* cusparse.jl: TODO: implement uniq! for sparse perceptron.

	* perceptron.jl: TODO: implement addx! for gpu perceptron.

	* usage.md: TODO: write loss documentation.

	* usage.md: TODO: write perceptron documentation.

	* param.jl: DONE: add initxavier for conv.jl

	* param.jl: TODO: fix adagrad initialization.

	* src/kperceptron.jl: TODO: make uniq! part of training somehow.

	* net.jl: profile train to see if gpumem hurts: it doesn't.  most
	of the time is spent during x2b.


2015-06-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/perceptron.jl: keep w the same orientation as x, makes
	easier to add.  Use w'*x like kperceptron.  Decided otherwise;
	wrong on both counts: w is dense, so not easier to add in either
	direction, and kperceptron holds it in w*k(x) position.

2015-06-23    <dyuret@ku.edu.tr>

	* src/kperceptron.jl: TODO: gpu/dense does not work with
	kpoly/kgauss yet.  gpu/sparse needs to be written.  Define new
	type CudaSparseMatrixCSC and test:
	- train (x2b,b2y)
	- initforw
	- forw
	- initback
	- back
	- update
	- hcat!
	- kpoly (gpu sparse/dense)
	- kgauss (gpu sparse/dense)

2015-06-22    <dyuret@ku.edu.tr>

	* DONE: xavier: https://github.com/BVLC/caffe/blob/master/include/caffe/filler.hpp#L129

2015-06-21    <dyuret@ku.edu.tr>

	* test/testkperceptron.jl:

	Q: linear kperceptron and perceptron do not give the same result?
	The difference is due to kperceptron not having a bias.  Removing
	bias from perceptron makes the results equal to numerical
	accuracy.

	Q: is K sparse or dense?  sparse.  +0 makes it dense.

	Q: should we add bias back to kperceptron but make it optional?
	it helps klinear.  does it help kpoly?  why does it hurt kgauss?

	Q: for loop is much slower than sparse mmul?

2015-06-20    <dyuret@ku.edu.tr>

	* kperceptron.jl: To move any further we need to sort out this
	array type business.  Since we introduced sparse arrays we are no
	longer limited to two types, Array and CudaArray.  That means the
	atype mechanism is no longer ok.

	First of all the original data comes in a cpu array.  It gets
	copied into minibatches by train and predict.  It can be full or
	sparse.  Nothing after the original data needs to be sparse except
	support vectors.  The minibatches could be on cpu or gpu (if gpu
	usage is specified).  We will keep the sparseness of the input in
	minibatches.  The users preferences should specify the cpu/gpu and
	the eltype that should be used internally in layer computations.
	Do we allow ftype to be different?  If not we could eliminate
	that and take it from the input as well?  So the input data
	determines the ftype and sparseness of layer calculations.  GPU
	used if present.  User has the option to turn gpu off.

	Conversions take place in param.jl, net.jl (train/predict).
	conv/pool only work with gpu arrays right now.
	perceptron/kperceptron only works with cpu arrays right now.

	So get rid of atype/ftype.  Get it from the input or during
	initialization.  So how do we initialize an mmul layer?  By
	specifying number of outputs.  Just like the perceptron.  The
	weight matrix gets constructed when the first input is received in
	initforw.  Mmul(array), Mmul(out), Mmul(out,in),
	Mmul(ftype,out,in) could be the initializers (modeled after
	zeros).  cpu/gpu is decided based on GPU setting (make it
	non-constant as before).  ftype defaults to Float64 as the rest of
	Julia (e.g. zeros(dims...), rand(dims...) etc).  So (out,in) can
	create a matrix.  Only (out) cannot, it will have to wait.  Param
	can play a more passive role?  Or we pass info to param?


2015-06-09    <dyuret@ku.edu.tr>

	* src/kernel.jl: Poly kernels working with sparse/full arrays.
	* DONE: Try on CudaArray: mul, xpose, hcat vs does not work.
	* DONE: Try on SparseCudaArray (cpu sparse is 5x slower than cpu
	full on mnist) -- postponed.
	* DONE: rbf kernel

2015-06-03    <dyuret@ku.edu.tr>

	* src/percloss.jl: Added perceptron loss.  A multiclass perceptron
	is just Mmul followed by PercLoss.
	* DONE: write unit test for PercLoss.
	* DONE: implement and test kernel perceptron next.
	* DONE: test subarray and concat with full/sparse on cpu/gpu.
	* DONE:	test KUnet with full/sparse, cpu/gpu, Float32/64 -- postponed
	* TODO: perform kuparser experiment comparing dense vs sparse
	features.
	* TODO: test onur's 4D code.
	* TODO: write doc on xent loss
	* TODO: write doc on perceptrons

2015-05-17    <dyuret@ku.edu.tr>

	* src/sigm.jl: done: make sure back returns dx.

	* src/logp.jl: done: gpu impl for back. why doesn't runtests catch
	this?  because dx=dy.

	* src/KUnet.jl: done: import copy etc.

	* src/net.jl:
	? add y=x and dx=dy optional args to forw and back.
	? the problem is do we copy if we don't modify?
	+ rename the options: fx=>dropout, dx=>returndx
	+ add o... options to all forw,back,copy,update,setparam!,loss

	* test/runtests.jl: done: cpu-only test.

	* test/runtests.jl: TODO: julia4 test.

	* src/param.jl: done: find a solution to copy.

2015-05-16    <dyuret@ku.edu.tr>

	* docs: TODO: update docs.

	* src/jldio.jl: done: update for new version.

	* src/net.jl: done: add shuffling back to train.

	* src/param.jl: done: compile cuda parts.

	* test/lenet.jl: done: need xavier init? the training does
	not take off until epoch 7 - turns out larger lr needed.

	* issimilar: done: add issimilar checks to all forw/back.

2015-05-15    <dyuret@ku.edu.tr>

	* test/runtests.jl: passed:
	+ bias.jl
	? conv.jl: only gpu, only 4D, no gradcheck
	+ drop.jl
	+ logp.jl
	+ logploss.jl
	+ mmul.jl
	? pool.jl: only gpu, only 4D, no gradcheck
	+ quadloss.jl
	+ relu.jl
	+ sigm.jl
	+ soft.jl
	+ softloss.jl
	+ tanh.jl
	+ xentloss.jl

	* src/xentloss.jl: Implementing loss functions as layers.  forw
	only records the outgoing y.  back takes the desired answer and
	overwrites it with the gradient of y wrt loss.  Note that this has
	two problems: (1) the actual loss value is never returned, i.e. we
	just compute gradients for training, use a separate loss function
	for eval. (2) the semantics of back is different: a regular
	layer's back takes dy, the loss gradient wrt output y, and returns
	dx, the loss gradient wrt input x.  A loss layer takes p, the gold
	answers, and returns dx, the loss gradient wrt input x.

2015-05-14    <dyuret@ku.edu.tr>

	* TODO:
	+ prob layer and loss fns: logploss, probloss, mseloss?, softmax?
	+ gradient check
	+ float64 support for Drop and Logp
	+ modify net.jl with the new loss convention.
	+ loss layers in gpu
	- conv pool gradcheck.
	- caffe comparison
	- conv pool cpu
	- conv pool 5D
	+ clean up float32:

	* test/runtests.jl: use Atype and Ftype to get gpu/cpu and
	float32/float64 behavior.

	* done: add a prob layer that computes normalized probabilities.
	Then rename three different softmax layers whether their input is
	unnormalized logp, logp, or prob.

2015-05-12    <dyuret@ku.edu.tr>

	* design: I have a new design:
	+ split every operation, including bias and activation.
	+ basically every operation in forw becomes its own "layer".
	+ each "layer" implements forw, back, update, setparam, copy.
	+ each "layer" overrides forw/back for arrays, cudaarrays, tensors.
	+ rename AbstractLayer -> Layer

2015-05-11    <dyuret@ku.edu.tr>

	* src/conv.jl:
	# TODO: How to transition from convolutional to fully connected layer?
	# Does a network pass around tensors with the same number of dimensions?
	# Can we write the code generic enough so it can deal with 2d matrices, 4d, 5d tensors?
	# In any case fc layer is different from conv layer...
	# n instances with c features is represented in caffe using a (1,1,C,N) tensor.
	# i.e. a 0-D image with C channels.
	# 2-D images are represented as (W,H,C,N).
	# 3-D images are represented as (D,W,H,C,N).
	# Is there any use for just (H,C,N)?
	# What is convolution for <2D or fc for >=2D?
	# Locality is important for convolution, i.e. dimensions other than
	# C,N give "neighboring" pixels so we can do convolution.
	# In a regular feature vector, there is no "neighbors" all features
	# are equally far from each other, that is why we use C,N.  There
	# would be no convolution operation in that case either.
	# N = instances in general (each instance leads to one class
	# prediction)
	# C = features in general
	# All other dimensions represent local neighborhoods.
	# So more generic data structure is reverse(N,C,I1,I2,I3,...)
	# And our regular layers are in fact 0-D, and conv can only be defined on >= 1-D.
	# We'd need to implement all except 2-D right now.
	# What does FC mean for >= 1-D?

	# TODO: do we really need the ConvolutionDescriptor

	# TODO: cudnn supports both Float32 and Float64, the rest of KUnet
	should too.

2015-03-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- Accept tuples for newnet and setparam to specify different
	values for different layers.  At least modify train.jl to be more
	similar to KUparser/test/train.jl.
	- sizeof, print for nets?
	- put an option for f of final layer (ffinal).
	+ add options to copy for testing (no need for training params)
	and saving (no need for transient fields).
	+ ERROR: CUDA runtime API library cannot be found - on yunus.
	- train.jl: allow length(v)==2*length(net) for param spec each w,b


2015-03-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- In KUnet, can we avoid reallocating everything unless we need more space?
	#    If batch gets smaller, just run the big batch through and copy part of the result?
	#    This needs some more thinking especially for training.


2015-03-01  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- make number type generic, test Float64
	- implement rmsprop: https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec6.pdf
	- implement adam: http://arxiv.org/pdf/1412.6980v2.pdf
	- understand adam math. what to do with g1?  what to do with g2?
	these are not stationary and our estimates are noisy.  what to do
	if we had perfect information?  does this correspond to newton
	with diagonal covariance matrix?  volkan's slides to adam email.
	- implement copynet cpu/gpu.
	- write parameter documentation.
	- implement hinge loss
	- implement squared loss
	- implement gradient clipping: pascanu and mikolov 2012.
	- implement rnn
	-- implement lstm
	-- implement gru
	-- steeper gates nips: lyu and zhu (piecewise linear)
	- orthogonal initialization: andrew sax
	- can we do piecewise linear approx to softmax? (hinge?)
	- try on machine with CUDArt but no gpu.

2015-02-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	+ start writing documentation.
	+ try install/test on a new gpu/cpu machine.
	+ build tests based on mnist.
	x compare with matlab/caffe if they exist.
	x what other tests?  gradient?  store answers?

2015-02-23  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/KUnet.jl:
	+ reconsider the constructors: they should only allow meaningful
	fields to be set, and they should call setparam for updateparams.
	- implement convnet: ConvLayer <: Layer
	+ centralize memory allocation
	- hdf5 save for whole net: use jld?

2015-02-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	x Make InplaceOps work without patch using CUBLAS generics.

2015-02-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:

	- implement/test maxout?
	- Write blogpost on overfitting (needs mnist)
	+ Write blogpost/README: deep learning in 250 lines of julia (needs mnist)
	+ Cleanup hdf5 files.
	+ Figure out automatic testing.
	+ Make softloss, get rid of soft layer.
	+ Add other losses
	+ make loss a training option.
	+ Add sigmoid layer.
	+ Make b and yforw conditional?
	+ Figure out if we have a gpu and if we are using a gpu, test code on no-gpu machine
	+ Export functions
	+ Make layer constructors that take size and generate random matrices
	+ Make layer constructors that take arbitrary matrices, h5 files
	x Error checking in cuda.jl
	+ pull request for InplaceOps
	+ pull request for CUBLAS
	+ pull request for ArgParse
	x Cleanup kernel calls in kunet.cu
	x Have kernel softmax return loss?
	x Cleanup hdf5 format in kunet_h5.cu, get rid of xfunc, yfunc,
	+ make dropout a layer option.
	+ Make train call backprop
	x implement/test maxnorm?
	+ use mnist for regression testing.

2015-02-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	- Verify generic functions in cuda.jl
	- Try to make update.jl more readable
	+ HDF5 should store the name of the functions
	+ Find a good way to handle dropout during training and testing.
	x maybe xforms should be part of the trainer not the layer.
	x caffe has it as another layer
	x i have tried as a separate fn or as part of forw/back before.

	+ implement/test dropout
	+ gpuseed not working, but random numbers start at the same place?
	  need to recreate RNG.
	+ cuda and julia not equal?
	x change dropout in cuda as well to use xmask for storage
	- change / check adagrad, nesterov options in cuda
	- implement/test maxnorm (cuda/matlab, no caffe test)

2015-02-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* DEBUG:
	+ test_fw_cuda: 2.26s
	+ test_fw_caffe: 3.82s
	+ test_fw_matlab: 3.83s
	+ test_fw_julia_cpu: 21.64s
	+ test_fw_julia_gpu: 5.39s ??? (check ger vs badd; do test with direct ccalls)
	+ who is allocating 35MB?
	+ elapsed time: 5.395230859 seconds (35 MB allocated, 0.06% gc time in 1 pauses with 0 full sweep)

2015-02-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	Possible design changes:
	+ Take training options out of Layer and pass them as options to layer update.
	+ That could support independent layer options but not w vs b.
	+ Group parameter and its diff in a blob like caffe: l->w->data, l->w->grad?
	x Make w and b two elements of an array: l->w[0,1]->ndims,dims,data,diff,diff1,diff2?
	x x and y have data,diff but no diff1 diff2.
	x But x has xmask, xones; we could use tmp1 and tmp2 as common names.
	+ Each w and b could have its own update options?
	+ Update can take each w, b individually, i.e. blob / options.
	x So can forward and back, take matrices instead of layers, but that's pushing it.
	+ To simplify memory management rely on xcols being correct in forw/drop and centralize alloc changes.

	+ figure out cuda rand to implement reproducible dropout.
	+ test dropout: figure out matlab seed, caffe layer.

2015-02-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	+ juliagpu implementation:
	+ inplace or devectorize macros adapted for gpu.
	x need to solve collections of options to c.
	x need to solve collections of arrays to c.
	+ there should be a generic julia implementation.
	+ the gpu support should be activated from the main script.

	+ speed test?
	+ momentum kernel, shrink code.
	x cpu/gpu blobs like caffe?  main goal: generic readable code.

	+ implement cuda/predict.
	+ implement cuda/backprop.
	+ implement cuda/train.
	+ implement data.h5 comparison.
	+ implement matlab/predict.
	+ compare cuda/predict to matlab.
	+ implement layer.h5 comparison.
	+ implement matlab/backprop.
	+ compare cuda/backprop to matlab. 
	+ implement matlab/train.
	+ compare cuda/train to matlab. 
	+ implement caffe/predict.
	+ implement caffe/backprop.
	+ implement caffe/train.
	+ compare cuda/predict to caffe.
	+ compare cuda/backprop to caffe. 
	+ compare cuda/train to caffe. 

	train options?
	+ already in file?
	+ take as cmd line opts?
	x try all variations?
	+ we'll need cmd-line opts at least for batch, epoch, etc.
	x (or assume epoch=1 and batch=100?)
	x yeah, simple train interface with train x l1 l2 .. y as well.
	x these are just test scripts after all.
	x maybe just do batch in the future.
	+ julia version:
	x layers fully opaque?
	+ train options?
	+ separate options from weights?
