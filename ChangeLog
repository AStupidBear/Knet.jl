2015-06-26  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/zn11perc64.jl: early convergence of regular perceptron is
	probably due to numerical accuracy.  Trying with Float64.  Fixed
	it.  Also added the (seed>0) condition to see unshuffled results.

	* TODO: run zn11orig with kperc and zn11single with perc as well.

	06261347-julia_zn11perc64_jl.out: perc+zn11orig
	- does not converge in 20 epochs but dev peaks.
	- does worse than Float32?
	best trn: (0,20,0.990577853561211,0.9628940312882318,0.9628049679369057)
	best dev: (0,12,0.9864501711719235,0.9629200614311373,0.9633301393086166)
	best tst: (0,13,0.9871840790335269,0.9627118202878934,0.9634038475713127)

	06261523-julia_zn11perc64b_jl.out: perc+zn11single
	- does much worse but does not peak in 20.
	best trn: (0,20,0.9607095614570927,0.9493974021917381,0.9487543303604334)
	best dev: (0,18,0.9596867048415946,0.9494234323346435,0.9485792732365298)
	best tst: (0,20,0.9607095614570927,0.9493974021917381,0.9487543303604334)

	06261407-zn11test.pbs.o12464157: kperceptron+zn11single.  kgauss,gamma=0.12.
	(1,(338122,124946),0.9608506650701513,0.9600224810201223)	trn=0.9313631349731266	time=3405
	(2,(338122,162316),0.9627899107166098,0.9627404732070465)	trn=0.9794714544999099	time=6740
	(3,(338122,180291),0.963336543717625,0.9635604776295422)	trn=0.9901257531344897	time=7920
	(4,(338122,191068),0.9638701616471874,0.9638921648116754)	trn=0.9940798465385478	time=8540
	(5,(338122,198841),0.9641174480047896,0.9640487948699049)	trn=0.995730040562692	time=8941
	(6,(338122,204862),0.9640784027904313,0.964076435468416)	trn=0.996692470632699	time=9242
	(7,(338122,209836),0.9640133274331676,0.9638000294833051)	trn=0.9972676214793297	time=9480
	(8,(338122,214092),0.9641304630762423,0.9638184565489791)	trn=0.9976620420217184	time=9679
	(9,(338122,217855),0.9641695082906006,0.9639013783445124)	trn=0.9979328628119658	time=9850

	06271100-julia_zn11test2_jl.out: kperceptron+zn11orig. kgauss,gamma=0.05.
	(1,(5123080,115857),0.9610589062133951,0.960206751676863)	trn=0.9363560156274033	time=5981
	(2,(5123080,146889),0.9629981518598537,0.9627957544040687)	trn=0.982953122184672	time=11998
	(3,(5123080,159776),0.9637009657183018,0.9636341858922385)	trn=0.992920755529578	time=13849
	(4,(5123080,167105),0.964195538433506,0.9639750866072087) 	trn=0.995973944073584	time=14723
	(5,(5123080,172184),0.9646250357914465,0.9640211542713938)	trn=0.997209941594997	time=15260
	(6,(5123080,176178),0.9645339302912773,0.964076435468416) 	trn=0.997805967066434	time=15659


2015-06-25  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/zn11test.pbs.o12463546: kperceptron on the zn11single
	feature set gives out of memory in epoch 2 with 125K sv while
	reallocating l.k.  (128*125K*4=64MB).  gc problem?  sv prealloc
	too aggressive?  stop reallocating l.k?  upload x and don't have
	sv?  sv costs ncols+2*nnz.  4*(125K+2*39*125K)=40MB.  Even double
	the size would be 80MB.  What is causing the error?  Do we need to
	call gc() during training?  Preallocate a huge sv, so we never
	alloc again?  Upload x would
	cost (2*nnz+1)*ncol*4=(2*39+1)*1.8M*4=569MB.  Would have to
	rewrite kernel.  Kernel caching would be easy using x indices.
	Shuffling would mess that up.  x could be indices too!  train
	could not handle it.  or train can just pass indices for x.  one
	hash (idx*idx->kval) plus a big x matrix in gpu memory for all the
	kernel calc.  what does a kernel cache hold again?  values or
	whole columns?  - It turns out gc() after every update fixes the
	problem.

	I calculated trn error looking at the number of sv added, and
	using the fact that nx=1820392.

	trn1: elapsed time: 3405.547185297 seconds (10105037976 bytes allocated, 9.32% gc time)
	dev1: elapsed time: 231.487048972 seconds (40981980 bytes allocated)
	tst1: elapsed time: 328.463520519 seconds (48170864 bytes allocated, 0.01% gc time)

	trn2: elapsed time: 6740.557072649 seconds (15279129024 bytes allocated, 4.77% gc time)
	dev2: elapsed time: 300.152283337 seconds (34083240 bytes allocated)
	tst2: elapsed time: 426.0930742 seconds (48177472 bytes allocated, 0.01% gc time)

	trn3: elapsed time: 7920.053201062 seconds (15272350808 bytes allocated, 4.07% gc time)
	dev3: elapsed time: 333.014117544 seconds (34072312 bytes allocated)
	tst3: elapsed time: 472.73413071 seconds (48172960 bytes allocated, 0.01% gc time)


	* test/zn11perc.pbs.o12463690: without shuffling, regular
	perceptron on the original zn11 feature set converges in 10 epochs
	to: (10,0.983861168363737,0.9627768956451571,0.9632195769145722)
	epoch,trn,dev,tst and stays there.

	* dbg/test/zn11perc.pbs.o12470963: zn11perc.jl: multi-seed
	shuffling version. The different seeds converge to slightly
	different solutions, but strangely each run converges and stops
	moving in exactly 10 epochs?  Other than that little funny
	coincidence the results are fairly consistent around:
	trn=9865 dev=9640 tst=9638

	These results are with Float32 and converge early.  Rerun with Float64.
	(seed,epoch,accuracy(ytrn,ztrn),accuracy(ydev,zdev),accuracy(ytst,ztst))
	(1,10,0.9866094775191278,0.9642606137907697,0.9638553106803273)
	(2,10,0.9866836373704125,0.9640784027904313,0.9637447482862829)
	(3,10,0.9865391629934651,0.9637139807897546,0.9638276700818161)
	(4,10,0.9865957442133343,0.9639742822188094,0.9639198054101865)
	(5,10,0.9865688269339791,0.9642606137907697,0.9638276700818161)
	(6,10,0.9865523469670269,0.9641174480047896,0.9639013783445124)
	(7,10,0.98665122676874,0.9638181013613765,0.9636526129579126)
	(8,10,0.986530923009989,0.964091417861884,0.9638553106803273)
	(9,10,0.9865122457141099,0.9636489054324908,0.9638276700818161)
	(10,10,0.9865122457141099,0.9638441315042819,0.9638368836146533)


2015-06-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/perceptron.jl: keep w the same orientation as x, makes
	easier to add.  Use w'*x like kperceptron.  Decided otherwise;
	wrong on both counts: w is dense, so not easier to add in either
	direction, and kperceptron holds it in w*k(x) position.

2015-06-23    <dyuret@ku.edu.tr>

	* src/kperceptron.jl: TODO: gpu/dense does not work with
	kpoly/kgauss yet.  gpu/sparse needs to be written.  Define new
	type CudaSparseMatrixCSC and test:
	- train (x2b,b2y)
	- initforw
	- forw
	- initback
	- back
	- update
	- hcat!
	- kpoly (gpu sparse/dense)
	- kgauss (gpu sparse/dense)

2015-06-22    <dyuret@ku.edu.tr>

	* TODO: xavier: https://github.com/BVLC/caffe/blob/master/include/caffe/filler.hpp#L129

2015-06-21    <dyuret@ku.edu.tr>

	* test/testkperceptron.jl:

	Q: linear kperceptron and perceptron do not give the same result?
	The difference is due to kperceptron not having a bias.  Removing
	bias from perceptron makes the results equal to numerical
	accuracy.

	Q: is K sparse or dense?  sparse.  +0 makes it dense.

	Q: should we add bias back to kperceptron but make it optional?
	it helps klinear.  does it help kpoly?  why does it hurt kgauss?

	Q: for loop is much slower than sparse mmul?

2015-06-20    <dyuret@ku.edu.tr>

	* kperceptron.jl: To move any further we need to sort out this
	array type business.  Since we introduced sparse arrays we are no
	longer limited to two types, Array and CudaArray.  That means the
	atype mechanism is no longer ok.

	First of all the original data comes in a cpu array.  It gets
	copied into minibatches by train and predict.  It can be full or
	sparse.  Nothing after the original data needs to be sparse except
	support vectors.  The minibatches could be on cpu or gpu (if gpu
	usage is specified).  We will keep the sparseness of the input in
	minibatches.  The users preferences should specify the cpu/gpu and
	the eltype that should be used internally in layer computations.
	Do we allow ftype to be different?  If not we could eliminate
	that and take it from the input as well?  So the input data
	determines the ftype and sparseness of layer calculations.  GPU
	used if present.  User has the option to turn gpu off.

	Conversions take place in param.jl, net.jl (train/predict).
	conv/pool only work with gpu arrays right now.
	perceptron/kperceptron only works with cpu arrays right now.

	So get rid of atype/ftype.  Get it from the input or during
	initialization.  So how do we initialize an mmul layer?  By
	specifying number of outputs.  Just like the perceptron.  The
	weight matrix gets constructed when the first input is received in
	initforw.  Mmul(array), Mmul(out), Mmul(out,in),
	Mmul(ftype,out,in) could be the initializers (modeled after
	zeros).  cpu/gpu is decided based on GPU setting (make it
	non-constant as before).  ftype defaults to Float64 as the rest of
	Julia (e.g. zeros(dims...), rand(dims...) etc).  So (out,in) can
	create a matrix.  Only (out) cannot, it will have to wait.  Param
	can play a more passive role?  Or we pass info to param?


2015-06-09    <dyuret@ku.edu.tr>

	* src/kernel.jl: Poly kernels working with sparse/full arrays.
	* DONE: Try on CudaArray: mul, xpose, hcat vs does not work.
	* DONE: Try on SparseCudaArray (cpu sparse is 5x slower than cpu
	full on mnist) -- postponed.
	* DONE: rbf kernel

2015-06-03    <dyuret@ku.edu.tr>

	* src/percloss.jl: Added perceptron loss.  A multiclass perceptron
	is just Mmul followed by PercLoss.
	* DONE: write unit test for PercLoss.
	* DONE: implement and test kernel perceptron next.
	* DONE: test subarray and concat with full/sparse on cpu/gpu.
	* DONE:	test KUnet with full/sparse, cpu/gpu, Float32/64 -- postponed
	* TODO: perform kuparser experiment comparing dense vs sparse
	features.
	* TODO: test onur's 4D code.
	* TODO: write doc on xent loss
	* TODO: write doc on perceptrons

2015-05-17    <dyuret@ku.edu.tr>

	* src/sigm.jl: done: make sure back returns dx.

	* src/logp.jl: done: gpu impl for back. why doesn't runtests catch
	this?  because dx=dy.

	* src/KUnet.jl: done: import copy etc.

	* src/net.jl:
	? add y=x and dx=dy optional args to forw and back.
	? the problem is do we copy if we don't modify?
	+ rename the options: fx=>dropout, dx=>returndx
	+ add o... options to all forw,back,copy,update,setparam!,loss

	* test/runtests.jl: done: cpu-only test.

	* test/runtests.jl: TODO: julia4 test.

	* src/param.jl: done: find a solution to copy.

2015-05-16    <dyuret@ku.edu.tr>

	* docs: TODO: update docs.

	* src/jldio.jl: done: update for new version.

	* src/net.jl: done: add shuffling back to train.

	* src/param.jl: done: compile cuda parts.

	* test/lenet.jl: done: need xavier init? the training does
	not take off until epoch 7 - turns out larger lr needed.

	* issimilar: done: add issimilar checks to all forw/back.

2015-05-15    <dyuret@ku.edu.tr>

	* test/runtests.jl: passed:
	+ bias.jl
	? conv.jl: only gpu, only 4D, no gradcheck
	+ drop.jl
	+ logp.jl
	+ logploss.jl
	+ mmul.jl
	? pool.jl: only gpu, only 4D, no gradcheck
	+ quadloss.jl
	+ relu.jl
	+ sigm.jl
	+ soft.jl
	+ softloss.jl
	+ tanh.jl
	+ xentloss.jl

	* src/xentloss.jl: Implementing loss functions as layers.  forw
	only records the outgoing y.  back takes the desired answer and
	overwrites it with the gradient of y wrt loss.  Note that this has
	two problems: (1) the actual loss value is never returned, i.e. we
	just compute gradients for training, use a separate loss function
	for eval. (2) the semantics of back is different: a regular
	layer's back takes dy, the loss gradient wrt output y, and returns
	dx, the loss gradient wrt input x.  A loss layer takes p, the gold
	answers, and returns dx, the loss gradient wrt input x.

2015-05-14    <dyuret@ku.edu.tr>

	* TODO:
	+ prob layer and loss fns: logploss, probloss, mseloss?, softmax?
	+ gradient check
	+ float64 support for Drop and Logp
	+ modify net.jl with the new loss convention.
	+ loss layers in gpu
	- conv pool gradcheck.
	- caffe comparison
	- conv pool cpu
	- conv pool 5D
	+ clean up float32:

	* test/runtests.jl: use Atype and Ftype to get gpu/cpu and
	float32/float64 behavior.

	* done: add a prob layer that computes normalized probabilities.
	Then rename three different softmax layers whether their input is
	unnormalized logp, logp, or prob.

2015-05-12    <dyuret@ku.edu.tr>

	* design: I have a new design:
	+ split every operation, including bias and activation.
	+ basically every operation in forw becomes its own "layer".
	+ each "layer" implements forw, back, update, setparam, copy.
	+ each "layer" overrides forw/back for arrays, cudaarrays, tensors.
	+ rename AbstractLayer -> Layer

2015-05-11    <dyuret@ku.edu.tr>

	* src/conv.jl:
	# TODO: How to transition from convolutional to fully connected layer?
	# Does a network pass around tensors with the same number of dimensions?
	# Can we write the code generic enough so it can deal with 2d matrices, 4d, 5d tensors?
	# In any case fc layer is different from conv layer...
	# n instances with c features is represented in caffe using a (1,1,C,N) tensor.
	# i.e. a 0-D image with C channels.
	# 2-D images are represented as (W,H,C,N).
	# 3-D images are represented as (D,W,H,C,N).
	# Is there any use for just (H,C,N)?
	# What is convolution for <2D or fc for >=2D?
	# Locality is important for convolution, i.e. dimensions other than
	# C,N give "neighboring" pixels so we can do convolution.
	# In a regular feature vector, there is no "neighbors" all features
	# are equally far from each other, that is why we use C,N.  There
	# would be no convolution operation in that case either.
	# N = instances in general (each instance leads to one class
	# prediction)
	# C = features in general
	# All other dimensions represent local neighborhoods.
	# So more generic data structure is reverse(N,C,I1,I2,I3,...)
	# And our regular layers are in fact 0-D, and conv can only be defined on >= 1-D.
	# We'd need to implement all except 2-D right now.
	# What does FC mean for >= 1-D?

	# TODO: do we really need the ConvolutionDescriptor

	# TODO: cudnn supports both Float32 and Float64, the rest of KUnet
	should too.

2015-03-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- Accept tuples for newnet and setparam to specify different
	values for different layers.  At least modify train.jl to be more
	similar to KUparser/test/train.jl.
	- sizeof, print for nets?
	- put an option for f of final layer (ffinal).
	+ add options to copy for testing (no need for training params)
	and saving (no need for transient fields).
	+ ERROR: CUDA runtime API library cannot be found - on yunus.
	- train.jl: allow length(v)==2*length(net) for param spec each w,b


2015-03-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- In KUnet, can we avoid reallocating everything unless we need more space?
	#    If batch gets smaller, just run the big batch through and copy part of the result?
	#    This needs some more thinking especially for training.


2015-03-01  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- make number type generic, test Float64
	- implement rmsprop: https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec6.pdf
	- implement adam: http://arxiv.org/pdf/1412.6980v2.pdf
	- understand adam math. what to do with g1?  what to do with g2?
	these are not stationary and our estimates are noisy.  what to do
	if we had perfect information?  does this correspond to newton
	with diagonal covariance matrix?  volkan's slides to adam email.
	- implement copynet cpu/gpu.
	- write parameter documentation.
	- implement hinge loss
	- implement squared loss
	- implement gradient clipping: pascanu and mikolov 2012.
	- implement rnn
	-- implement lstm
	-- implement gru
	-- steeper gates nips: lyu and zhu (piecewise linear)
	- orthogonal initialization: andrew sax
	- can we do piecewise linear approx to softmax? (hinge?)
	- try on machine with CUDArt but no gpu.

2015-02-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	+ start writing documentation.
	+ try install/test on a new gpu/cpu machine.
	- build tests based on mnist.
	- compare with matlab/caffe if they exist.
	- what other tests?  gradient?  store answers?

2015-02-23  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/KUnet.jl:
	+ reconsider the constructors: they should only allow meaningful
	fields to be set, and they should call setparam for updateparams.
	- implement convnet: ConvLayer <: Layer
	+ centralize memory allocation
	- hdf5 save for whole net: use jld?

2015-02-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- Make InplaceOps work without patch using CUBLAS generics.

2015-02-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:

	+ Write blogpost/README: deep learning in 250 lines of julia (needs mnist)
	- Write blogpost on overfitting (needs mnist)
	- Cleanup hdf5 files.
	- Figure out automatic testing.
	+ Make softloss, get rid of soft layer.
	- Add other losses
	+ make loss a training option.
	- Add sigmoid layer.
	+ Make b and yforw conditional?
	+ Figure out if we have a gpu and if we are using a gpu, test code on no-gpu machine
	+ Export functions
	+ Make layer constructors that take size and generate random matrices
	+ Make layer constructors that take arbitrary matrices, h5 files
	- Error checking in cuda.jl
	+ pull request for InplaceOps
	+ pull request for CUBLAS
	+ pull request for ArgParse
	- Cleanup kernel calls in kunet.cu
	- Have kernel softmax return loss?
	- Cleanup hdf5 format in kunet_h5.cu, get rid of xfunc, yfunc,
	+ make dropout a layer option.
	+ Make train call backprop
	- implement/test maxnorm?
	- implement/test maxout?
	- use mnist for regression testing.

2015-02-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- Verify generic functions in cuda.jl
	- Try to make update.jl more readable
	+ HDF5 should store the name of the functions
	+ Find a good way to handle dropout during training and testing.
	x maybe xforms should be part of the trainer not the layer.
	x caffe has it as another layer
	x i have tried as a separate fn or as part of forw/back before.

	+ implement/test dropout
	- gpuseed not working, but random numbers start at the same place?
	+ cuda and julia not equal?
	x change dropout in cuda as well to use xmask for storage
	- change / check adagrad, nesterov options in cuda
	- implement/test maxnorm (cuda/matlab, no caffe test)

2015-02-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* DEBUG:
	+ test_fw_cuda: 2.26s
	+ test_fw_caffe: 3.82s
	+ test_fw_matlab: 3.83s
	+ test_fw_julia_cpu: 21.64s
	+ test_fw_julia_gpu: 5.39s ??? (check ger vs badd; do test with direct ccalls)
	+ who is allocating 35MB?
	+ elapsed time: 5.395230859 seconds (35 MB allocated, 0.06% gc time in 1 pauses with 0 full sweep)

2015-02-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	Possible design changes:
	+ Take training options out of Layer and pass them as options to layer update.
	+ That could support independent layer options but not w vs b.
	+ Group parameter and its diff in a blob like caffe: l->w->data, l->w->grad?
	x Make w and b two elements of an array: l->w[0,1]->ndims,dims,data,diff,diff1,diff2?
	x x and y have data,diff but no diff1 diff2.
	x But x has xmask, xones; we could use tmp1 and tmp2 as common names.
	+ Each w and b could have its own update options?
	+ Update can take each w, b individually, i.e. blob / options.
	x So can forward and back, take matrices instead of layers, but that's pushing it.
	+ To simplify memory management rely on xcols being correct in forw/drop and centralize alloc changes.

	+ figure out cuda rand to implement reproducible dropout.
	+ test dropout: figure out matlab seed, caffe layer.

2015-02-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	+ juliagpu implementation:
	+ inplace or devectorize macros adapted for gpu.
	x need to solve collections of options to c.
	x need to solve collections of arrays to c.
	+ there should be a generic julia implementation.
	+ the gpu support should be activated from the main script.

	+ speed test?
	+ momentum kernel, shrink code.
	x cpu/gpu blobs like caffe?  main goal: generic readable code.

	+ implement cuda/predict.
	+ implement cuda/backprop.
	+ implement cuda/train.
	+ implement data.h5 comparison.
	+ implement matlab/predict.
	+ compare cuda/predict to matlab.
	+ implement layer.h5 comparison.
	+ implement matlab/backprop.
	+ compare cuda/backprop to matlab. 
	+ implement matlab/train.
	+ compare cuda/train to matlab. 
	+ implement caffe/predict.
	+ implement caffe/backprop.
	+ implement caffe/train.
	+ compare cuda/predict to caffe.
	+ compare cuda/backprop to caffe. 
	+ compare cuda/train to caffe. 

	train options?
	+ already in file?
	+ take as cmd line opts?
	x try all variations?
	+ we'll need cmd-line opts at least for batch, epoch, etc.
	x (or assume epoch=1 and batch=100?)
	x yeah, simple train interface with train x l1 l2 .. y as well.
	x these are just test scripts after all.
	x maybe just do batch in the future.
	+ julia version:
	x layers fully opaque?
	+ train options?
	+ separate options from weights?
