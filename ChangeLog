2015-05-14    <dyuret@ku.edu.tr>

	* TODO:
	- gradient check
	- prob layer and loss fns: logploss, probloss, mseloss?, softmax?
	- float64 support for Drop and Logp
	- conv pool cpu
	- conv pool 5D
	- clean up float32:
	drop.jl:27:drop(x::CudaArray, xdrop::CudaArray, dropout, scale)=ccall((:drop,libkunet),Void,(Cint,Cmat,Cmat,Cfloat,Cfloat),length(x),x,xdrop,dropout,scale)
	logp.jl:30:    ccall((:logpforw,libkunet),Void,(Cint,Cint,Cmat),size(y,1),size(y,2),y)
	loss.jl:49:softmaxloss(y::CudaArray,dy::CudaArray)=ccall((:softback,libkunet),Cfloat,(Cint,Cint,Cmat,Cmat),size(dy,1),size(dy,2),y,dy)
	loss.jl:50:logploss(y::CudaArray,dy::CudaArray)=ccall((:logploss,libkunet),Cfloat,(Cint,Cint,Cmat,Cmat),size(dy,1),size(dy,2),y,dy)
	param.jl:33:    adagrad!(eps, dw2::CudaArray, dw::CudaArray)=ccall((:adagrad,libkunet),Void,(Cint,Cfloat,Cmat,Cmat),length(dw),eps,dw2,dw)
	param.jl:34:    l1reg!(l1, w::CudaArray, dw::CudaArray)=ccall((:l1reg,libkunet),Void,(Cint,Cfloat,Cmat,Cmat),length(dw),l1,w,dw)
	relu.jl:11:#forw(l::Relu,x::CudaArray; o...)=(ccall((:reluforw,libkunet),Void,(Cint,Cmat),length(x),x); l.y=x)
	relu.jl:12:#back(l::Relu,dy::CudaArray; o...)=(ccall((:reluback,libkunet),Void,(Cint,Cmat,Cmat),length(dy),l.y,dy); dy)
	util.jl:14:typealias Cmat Ptr{Float32}
	util.jl:28:InplaceOps.badd!(::Type{InplaceOps.Inplace{1}}, A::CudaMatrix, B::CudaVecOrMat) = (ccall((:badd,libkunet),Void,(Cint,Cint,Cmat,Cmat),size(A,1),size(A,2),A,B);A) # InplaceOps.jl:83
	util.jl:29:InplaceOps.bmul!(::Type{InplaceOps.Inplace{1}}, A::CudaArray, x::Number) = CUBLAS.scal!(length(A), float32(x), A, 1)
	util.jl:30:InplaceOps.bsub!(::Type{InplaceOps.Inplace{1}}, A::CudaArray, B::CudaArray) = CUBLAS.axpy!(length(A), -1.0f0, B, 1, A, 1)
	util.jl:31:InplaceOps.bsub!(::Type{InplaceOps.Inplace{1}}, A::CudaArray, x::Number) = (ccall((:add1,libkunet),Void,(Cint,Cfloat,Cmat),length(A),-x,A);A)
	util.jl:39:Base.sum!(r::CudaVecOrMat, A::CudaMatrix) = ccall((:bsum,libkunet),Void,(Cint,Cint,Cmat,Cmat),size(A,1),size(A,2),A,r) # reducedim.jl:226
	util.jl:41:Base.rand!(A::CudaArray)=(ccall((:randfill,libkunet),Void,(Cint,Cmat),length(A),A); A)
	util.jl:42:Base.fill!(A::CudaArray,x::Number)=(ccall((:fill,libkunet),Void,(Cint,Cfloat,Cmat),length(A),x,A); A)

	* test/runtests.jl: use Atype and Ftype to get gpu/cpu and
	float32/float64 behavior?

	* TODO: add a prob layer that computes normalized probabilities.
	Then rename three different softmax layers whether their input is
	unnormalized logp, logp, or prob.

2015-05-12    <dyuret@ku.edu.tr>

	* design: I have a new design:
	- split every operation, including bias and activation.
	- basically every operation in forw becomes its own "layer".
	- each "layer" implements forw, back, update?
	- each "layer" overrides forw/back for arrays, cudaarrays,
	tensors.
	- rename AbstractLayer -> Layer

2015-05-11    <dyuret@ku.edu.tr>

	* src/conv.jl:
	# TODO: How to transition from convolutional to fully connected layer?
	# Does a network pass around tensors with the same number of dimensions?
	# Can we write the code generic enough so it can deal with 2d matrices, 4d, 5d tensors?
	# In any case fc layer is different from conv layer...
	# n instances with c features is represented in caffe using a (1,1,C,N) tensor.
	# i.e. a 0-D image with C channels.
	# 2-D images are represented as (W,H,C,N).
	# 3-D images are represented as (D,W,H,C,N).
	# Is there any use for just (H,C,N)?
	# What is convolution for <2D or fc for >=2D?
	# Locality is important for convolution, i.e. dimensions other than
	# C,N give "neighboring" pixels so we can do convolution.
	# In a regular feature vector, there is no "neighbors" all features
	# are equally far from each other, that is why we use C,N.  There
	# would be no convolution operation in that case either.
	# N = instances in general (each instance leads to one class
	# prediction)
	# C = features in general
	# All other dimensions represent local neighborhoods.
	# So more generic data structure is reverse(N,C,I1,I2,I3,...)
	# And our regular layers are in fact 0-D, and conv can only be defined on >= 1-D.
	# We'd need to implement all except 2-D right now.
	# What does FC mean for >= 1-D?

	# TODO: do we really need the ConvolutionDescriptor

	# TODO: cudnn supports both Float32 and Float64, the rest of KUnet
	should too.

2015-03-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- Accept tuples for newnet and setparam to specify different
	values for different layers.  At least modify train.jl to be more
	similar to KUparser/test/train.jl.
	- sizeof, print for nets?
	- put an option for f of final layer (ffinal).
	+ add options to copy for testing (no need for training params)
	and saving (no need for transient fields).
	+ ERROR: CUDA runtime API library cannot be found - on yunus.
	- train.jl: allow length(v)==2*length(net) for param spec each w,b


2015-03-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- In KUnet, can we avoid reallocating everything unless we need more space?
	#    If batch gets smaller, just run the big batch through and copy part of the result?
	#    This needs some more thinking especially for training.


2015-03-01  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- make number type generic, test Float64
	- implement rmsprop: https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec6.pdf
	- implement adam: http://arxiv.org/pdf/1412.6980v2.pdf
	- understand adam math. what to do with g1?  what to do with g2?
	these are not stationary and our estimates are noisy.  what to do
	if we had perfect information?  does this correspond to newton
	with diagonal covariance matrix?  volkan's slides to adam email.
	- implement copynet cpu/gpu.
	- write parameter documentation.
	- implement hinge loss
	- implement squared loss
	- implement gradient clipping: pascanu and mikolov 2012.
	- implement rnn
	-- implement lstm
	-- implement gru
	-- steeper gates nips: lyu and zhu (piecewise linear)
	- orthogonal initialization: andrew sax
	- can we do piecewise linear approx to softmax? (hinge?)
	- try on machine with CUDArt but no gpu.

2015-02-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	+ start writing documentation.
	+ try install/test on a new gpu/cpu machine.
	- build tests based on mnist.
	- compare with matlab/caffe if they exist.
	- what other tests?  gradient?  store answers?

2015-02-23  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/KUnet.jl:
	+ reconsider the constructors: they should only allow meaningful
	fields to be set, and they should call setparam for updateparams.
	- implement convnet: ConvLayer <: Layer
	+ centralize memory allocation
	- hdf5 save for whole net: use jld?

2015-02-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- Make InplaceOps work without patch using CUBLAS generics.

2015-02-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:

	+ Write blogpost/README: deep learning in 250 lines of julia (needs mnist)
	- Write blogpost on overfitting (needs mnist)
	- Cleanup hdf5 files.
	- Figure out automatic testing.
	+ Make softloss, get rid of soft layer.
	- Add other losses
	+ make loss a training option.
	- Add sigmoid layer.
	+ Make b and yforw conditional?
	+ Figure out if we have a gpu and if we are using a gpu, test code on no-gpu machine
	+ Export functions
	+ Make layer constructors that take size and generate random matrices
	+ Make layer constructors that take arbitrary matrices, h5 files
	- Error checking in cuda.jl
	+ pull request for InplaceOps
	+ pull request for CUBLAS
	+ pull request for ArgParse
	- Cleanup kernel calls in kunet.cu
	- Have kernel softmax return loss?
	- Cleanup hdf5 format in kunet_h5.cu, get rid of xfunc, yfunc,
	+ make dropout a layer option.
	+ Make train call backprop
	- implement/test maxnorm?
	- implement/test maxout?
	- use mnist for regression testing.

2015-02-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- Verify generic functions in cuda.jl
	- Try to make update.jl more readable
	+ HDF5 should store the name of the functions
	+ Find a good way to handle dropout during training and testing.
	x maybe xforms should be part of the trainer not the layer.
	x caffe has it as another layer
	x i have tried as a separate fn or as part of forw/back before.

	+ implement/test dropout
	- gpuseed not working, but random numbers start at the same place?
	+ cuda and julia not equal?
	x change dropout in cuda as well to use xmask for storage
	- change / check adagrad, nesterov options in cuda
	- implement/test maxnorm (cuda/matlab, no caffe test)

2015-02-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* DEBUG:
	+ test_fw_cuda: 2.26s
	+ test_fw_caffe: 3.82s
	+ test_fw_matlab: 3.83s
	+ test_fw_julia_cpu: 21.64s
	+ test_fw_julia_gpu: 5.39s ??? (check ger vs badd; do test with direct ccalls)
	+ who is allocating 35MB?
	+ elapsed time: 5.395230859 seconds (35 MB allocated, 0.06% gc time in 1 pauses with 0 full sweep)

2015-02-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	Possible design changes:
	+ Take training options out of Layer and pass them as options to layer update.
	+ That could support independent layer options but not w vs b.
	+ Group parameter and its diff in a blob like caffe: l->w->data, l->w->grad?
	x Make w and b two elements of an array: l->w[0,1]->ndims,dims,data,diff,diff1,diff2?
	x x and y have data,diff but no diff1 diff2.
	x But x has xmask, xones; we could use tmp1 and tmp2 as common names.
	+ Each w and b could have its own update options?
	+ Update can take each w, b individually, i.e. blob / options.
	x So can forward and back, take matrices instead of layers, but that's pushing it.
	+ To simplify memory management rely on xcols being correct in forw/drop and centralize alloc changes.

	+ figure out cuda rand to implement reproducible dropout.
	+ test dropout: figure out matlab seed, caffe layer.

2015-02-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	+ juliagpu implementation:
	+ inplace or devectorize macros adapted for gpu.
	x need to solve collections of options to c.
	x need to solve collections of arrays to c.
	+ there should be a generic julia implementation.
	+ the gpu support should be activated from the main script.

	+ speed test?
	+ momentum kernel, shrink code.
	x cpu/gpu blobs like caffe?  main goal: generic readable code.

	+ implement cuda/predict.
	+ implement cuda/backprop.
	+ implement cuda/train.
	+ implement data.h5 comparison.
	+ implement matlab/predict.
	+ compare cuda/predict to matlab.
	+ implement layer.h5 comparison.
	+ implement matlab/backprop.
	+ compare cuda/backprop to matlab. 
	+ implement matlab/train.
	+ compare cuda/train to matlab. 
	+ implement caffe/predict.
	+ implement caffe/backprop.
	+ implement caffe/train.
	+ compare cuda/predict to caffe.
	+ compare cuda/backprop to caffe. 
	+ compare cuda/train to caffe. 

	train options?
	+ already in file?
	+ take as cmd line opts?
	x try all variations?
	+ we'll need cmd-line opts at least for batch, epoch, etc.
	x (or assume epoch=1 and batch=100?)
	x yeah, simple train interface with train x l1 l2 .. y as well.
	x these are just test scripts after all.
	x maybe just do batch in the future.
	+ julia version:
	x layers fully opaque?
	+ train options?
	+ separate options from weights?
