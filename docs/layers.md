## Layers

* [Bias](https://github.com/denizyuret/KUnet.jl/blob/master/src/bias.jl)
* [Conv](https://github.com/denizyuret/KUnet.jl/blob/master/src/conv.jl)
* [Drop](https://github.com/denizyuret/KUnet.jl/blob/master/src/drop.jl)
* [Logp](https://github.com/denizyuret/KUnet.jl/blob/master/src/logp.jl)
* [Mmul](https://github.com/denizyuret/KUnet.jl/blob/master/src/mmul.jl)
* [Pool](https://github.com/denizyuret/KUnet.jl/blob/master/src/pool.jl)
* [Relu](https://github.com/denizyuret/KUnet.jl/blob/master/src/relu.jl)
* [Sigm](https://github.com/denizyuret/KUnet.jl/blob/master/src/sigm.jl)
* [Soft](https://github.com/denizyuret/KUnet.jl/blob/master/src/soft.jl)
* [Tanh](https://github.com/denizyuret/KUnet.jl/blob/master/src/tanh.jl)

See [Loss Layers](loss.md) for layers implementing loss functions and
[Perceptron](perceptron.md) for stand-alone layers implementing
perceptrons and kernel perceptrons.
