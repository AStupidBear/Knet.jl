## Layers

To be written...

* [Mmul](https://github.com/denizyuret/Knet.jl/blob/master/src/mmul.jl)
* [Bias](https://github.com/denizyuret/Knet.jl/blob/master/src/bias.jl)

* [Conv](https://github.com/denizyuret/Knet.jl/blob/master/src/conv.jl)
* [Pool](https://github.com/denizyuret/Knet.jl/blob/master/src/pool.jl)

* [Drop](https://github.com/denizyuret/Knet.jl/blob/master/src/drop.jl)

* [Logp](https://github.com/denizyuret/Knet.jl/blob/master/src/logp.jl)
* [Relu](https://github.com/denizyuret/Knet.jl/blob/master/src/relu.jl)
* [Sigm](https://github.com/denizyuret/Knet.jl/blob/master/src/sigm.jl)
* [Soft](https://github.com/denizyuret/Knet.jl/blob/master/src/soft.jl)
* [Tanh](https://github.com/denizyuret/Knet.jl/blob/master/src/tanh.jl)

See [Loss Layers](loss.md) for layers implementing loss functions and
[Perceptrons](perceptron.md) for stand-alone layers implementing
perceptrons and kernel perceptrons.
