<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · Knet.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/default.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Ubuntu+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="assets/documenter.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="../versions.js"></script></head><body><nav class="toc"><h1>Knet.jl</h1><form class="search" action="search.html"><select id="version-selector" onChange="window.location.href=this.value"><option value="#" selected="selected" disabled="disabled">Version</option></select><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="install.html">Setting up Knet</a></li><li><a class="toctext" href="tutorial.html">Introduction to Knet</a></li><li><a class="toctext" href="examples.html">Examples</a></li><li class="current"><a class="toctext" href="reference.html">Reference</a><ul class="internal"><li><a class="toctext" href="#AutoGrad-1">AutoGrad</a></li><li><a class="toctext" href="#KnetArray-1">KnetArray</a></li><li><a class="toctext" href="#Utilities-1">Utilities</a></li><li><a class="toctext" href="#Convolution-1">Convolution</a></li><li><a class="toctext" href="#Optimization-1">Optimization</a></li><li><a class="toctext" href="#Initialization-1">Initialization</a></li><li><a class="toctext" href="#AutoGrad-(advanced)-1">AutoGrad (advanced)</a></li><li><a class="toctext" href="#Function-Index-1">Function Index</a></li></ul></li></ul></li><li><span class="toctext">Textbook</span><ul><li><a class="toctext" href="backprop.html">Backpropagation</a></li><li><a class="toctext" href="softmax.html">Softmax Classification</a></li><li><a class="toctext" href="mlp.html">Multilayer Perceptrons</a></li><li><a class="toctext" href="cnn.html">Convolutional Neural Networks</a></li><li><a class="toctext" href="rnn.html">Recurrent Neural Networks</a></li><li><a class="toctext" href="rl.html">Reinforcement Learning</a></li><li><a class="toctext" href="opt.html">Optimization</a></li><li><a class="toctext" href="gen.html">Generalization</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li><a href="reference.html">Reference</a></li></ul><a class="edit-page" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/docs/src/reference.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/></header><h1><a class="nav-anchor" id="Reference-1" href="#Reference-1">Reference</a></h1><p><strong>Contents</strong></p><ul><li><a href="reference.html#Reference-1">Reference</a></li><ul><li><a href="reference.html#AutoGrad-1">AutoGrad</a></li><li><a href="reference.html#KnetArray-1">KnetArray</a></li><li><a href="reference.html#Utilities-1">Utilities</a></li><li><a href="reference.html#Convolution-1">Convolution</a></li><li><a href="reference.html#Optimization-1">Optimization</a></li><li><a href="reference.html#Initialization-1">Initialization</a></li><li><a href="reference.html#AutoGrad-(advanced)-1">AutoGrad (advanced)</a></li><li><a href="reference.html#Function-Index-1">Function Index</a></li></ul></ul><h2><a class="nav-anchor" id="AutoGrad-1" href="#AutoGrad-1">AutoGrad</a></h2><pre><code class="language-none">AutoGrad.grad
AutoGrad.gradloss
AutoGrad.gradcheck</code></pre><h2><a class="nav-anchor" id="KnetArray-1" href="#KnetArray-1">KnetArray</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.KnetArray" href="#Knet.KnetArray"><code>Knet.KnetArray</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">KnetArray(T, dims)
KnetArray(a::AbstractArray)
Array(k::KnetArray)</code></pre><p>Container for GPU arrays that supports most of the AbstractArray interface.  KnetArrays and Arrays can be converted to each other as shown above, which involves copying to and from the GPU memory. Important differences from the alternative CudaArray are: (1) a custom memory manager that minimizes the number of calls to the slow cudaMalloc by reusing already allocated but garbage collected GPU pointers.  (2) a custom getindex that handles ranges such as <code>a[5:10]</code> as views with shared memory instead of copies.</p><p><strong>Supported functions:</strong></p><ul><li><p>Array operations: cat, convert, copy, display, eachindex, elsize, eltype, endof, fill!, first, getindex, hcat, isempty, length, linearindexing, ndims, ones, pointer, rand!, reshape, setindex!, similar, size, stride, summary, vcat, vec, zeros</p></li><li><p>Math operators: (-), abs, abs2, acos, acosh, asin, asinh, atan, atanh, cbrt, ceil, cos, cosh, cospi, erf, erfc, erfcinv, erfcx, erfinv, exp, exp10, exp2, expm1, floor, log, log10, log1p, log2, round, sign, sin, sinh, sinpi, sqrt, tan, tanh, trunc</p></li><li><p>Broadcasting operators: (.*), (.+), (.-), (./), (.&lt;), (.&lt;=), (.!=), (.==), (.&gt;), (.&gt;=), (.^), max, min.  (Only Array-Scalar and Array-Vector broadcasting are supported)</p></li><li><p>Reduction operators: countnz, maximum, minimum, prod, sum, sumabs, sumabs2, vecnorm.  (Only Array-&gt;Scalar and Array-&gt;Vector reductions are supported)</p></li><li><p>Linear algebra: (*), axpy!, permutedims (only 2D and 3D), transpose</p></li><li><p>Knet extras: cpu2gpu, gpu2cpu, relu, sigm, invx, logp, logsumexp, conv4, pool, deconv4, unpool, mat, update!</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/karray.jl#L1-L42">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.cpu2gpu" href="#Knet.cpu2gpu"><code>Knet.cpu2gpu</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Transfer from regular Array to KnetArray.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/karray.jl#L425-L427">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.gpu2cpu" href="#Knet.gpu2cpu"><code>Knet.gpu2cpu</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Transfer from KnetArray to regular Array.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/karray.jl#L434-L436">source</a><br/></section><h2><a class="nav-anchor" id="Utilities-1" href="#Utilities-1">Utilities</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.dir" href="#Knet.dir"><code>Knet.dir</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Knet.dir(path...)</code></pre><p>Construct a path relative to Knet root.</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; Knet.dir(&quot;examples&quot;,&quot;mnist.jl&quot;)
&quot;/home/dyuret/.julia/v0.5/Knet/examples/mnist.jl&quot;</code></pre></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/Knet.jl#L21-L31">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.gpu" href="#Knet.gpu"><code>Knet.gpu</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>gpu()</code> returns the id of the active GPU device or -1 if none are active.</p><p><code>gpu(true)</code> resets all GPU devices and activates the one with the most available memory.</p><p><code>gpu(false)</code> resets and deactivates all GPU devices.</p><p><code>gpu(d::Int)</code> activates the GPU device <code>d</code> if <code>0 &lt;= d &lt; gpuCount()</code>, otherwise deactivates devices.</p><p><code>gpu(true/false)</code> resets all devices.  If there are any allocated KnetArrays their pointers will be left dangling.  Thus <code>gpu(true/false)</code> should only be used during startup.  If you want to suspend GPU use temporarily, use <code>gpu(-1)</code>.</p><p><code>gpu(d::Int)</code> does not reset the devices.  You can select a previous device and find allocated memory preserved.  However trying to operate on arrays of an inactive device will result in error.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/gpu.jl#L15-L37">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.logp" href="#Knet.logp"><code>Knet.logp</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">logp(x,[dims])</code></pre><p>Treat entries in <code>x</code> as as unnormalized log probabilities and return normalized log probabilities.</p><p><code>dims</code> is an optional argument, if not specified the normalization is over the whole <code>x</code>, otherwise the normalization is performed over the given dimensions.  In particular, if <code>x</code> is a matrix, <code>dims=1</code> normalizes columns of <code>x</code> and <code>dims=2</code> normalizes rows of <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/unary.jl#L139-L151">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.logsumexp" href="#Knet.logsumexp"><code>Knet.logsumexp</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">logsumexp(x,[dims])</code></pre><p>Compute <code>log(sum(exp(x),dims))</code> in a numerically stable manner.</p><p><code>dims</code> is an optional argument, if not specified the summation is over the whole <code>x</code>, otherwise the summation is performed over the given dimensions.  In particular if <code>x</code> is a matrix, <code>dims=1</code> sums columns of <code>x</code> and <code>dims=2</code> sums rows of <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/reduction.jl#L91-L102">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.invx" href="#Knet.invx"><code>Knet.invx</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>invx(x) = (1./x)</code></p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/unary.jl#L129">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.relu" href="#Knet.relu"><code>Knet.relu</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>relu(x) = max(0,x)</code></p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/unary.jl#L130">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.sigm" href="#Knet.sigm"><code>Knet.sigm</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>sigm(x) = (1./(1+exp(-x)))</code></p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/unary.jl#L131">source</a><br/></section><h2><a class="nav-anchor" id="Convolution-1" href="#Convolution-1">Convolution</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.conv4" href="#Knet.conv4"><code>Knet.conv4</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">conv4(w, x; kwargs...)</code></pre><p>Execute convolutions or cross-correlations using filters specified with <code>w</code> over tensor <code>x</code>.</p><p>Currently 4 or 5 dimensional KnetArrays with <code>Float32</code> or <code>Float64</code> entries are supported.  If <code>w</code> has dimensions <code>(W1,W2,...,I,O)</code> and <code>x</code> has dimensions <code>(X1,X2,...,I,N)</code>, the result <code>y</code> will have dimensions <code>(Y1,Y2,...,O,N)</code> where</p><pre><code class="language-none">Yi=1+floor((Xi+2*padding[i]-Wi)/stride[i])</code></pre><p>Here <code>I</code> is the number of input channels, <code>O</code> is the number of output channels, <code>N</code> is the number of instances, and <code>Wi,Xi,Yi</code> are spatial dimensions.  <code>padding</code> and <code>stride</code> are keyword arguments that can be specified as a single number (in which case they apply to all dimensions), or an array/tuple with entries for each spatial dimension.</p><p><strong>Keywords</strong></p><ul><li><p><code>padding=0</code>: the number of extra zeros implicitly concatenated at the start and at the end of each dimension.</p></li><li><p><code>stride=1</code>: the number of elements to slide to reach the next filtering window.</p></li><li><p><code>upscale=1</code>: upscale factor for each dimension.</p></li><li><p><code>mode=0</code>: 0 for convolution and 1 for cross-correlation.</p></li><li><p><code>alpha=1</code>: can be used to scale the result.</p></li><li><p><code>algo=0</code>: specifies which convolution algorithm shoud be used to compute the results. See the CUDNN User Guide for details.</p></li><li><p><code>workSpace=C_NULL</code>: data pointer to GPU memory to a workspace needed to able to execute the specified algorithm.</p></li><li><p><code>workSpaceSizeInBytes=0</code>: the size in bytes of the provided workSpace. Default=0.</p></li><li><p><code>handle</code>: handle to a previously created cuDNN context. Defaults to a Knet allocated handle.</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/conv.jl#L3-L36">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.pool" href="#Knet.pool"><code>Knet.pool</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">pool(x; kwargs...)</code></pre><p>Compute pooling of input values (i.e., the maximum or average of several adjacent values) to produce an output with smaller height and/or width.  </p><p>Currently 4 or 5 dimensional KnetArrays with <code>Float32</code> or <code>Float64</code> entries are supported.  If <code>x</code> has dimensions <code>(X1,X2,...,I,N)</code>, the result <code>y</code> will have dimensions <code>(Y1,Y2,...,I,N)</code> where</p><pre><code class="language-none">Yi=1+floor((Xi+2*padding[i]-window[i])/stride[i])</code></pre><p>Here <code>I</code> is the number of input channels, <code>N</code> is the number of instances, and <code>Xi,Yi</code> are spatial dimensions.  <code>window</code>, <code>padding</code> and <code>stride</code> are keyword arguments that can be specified as a single number (in which case they apply to all dimensions), or an array/tuple with entries for each spatial dimension.</p><p><strong>Keywords:</strong></p><ul><li><p><code>window=2</code>: the pooling window size for each dimension.</p></li><li><p><code>padding=0</code>: the number of extra zeros implicitly concatenated at the start and at the end of each dimension.</p></li><li><p><code>stride=window</code>: the number of elements to slide to reach the next pooling window.</p></li><li><p><code>mode=0</code>: 0 for max, 1 for average including padded values, 2 for average excluding padded values.</p></li><li><p><code>maxpoolingNanOpt=0</code>: Nan numbers are not propagated if 0, they are propagated if 1.</p></li><li><p><code>alpha=1</code>: can be used to scale the result.</p></li><li><p><code>handle</code>: Handle to a previously created cuDNN context. Defaults to a Knet allocated handle.</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/conv.jl#L93-L123">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.mat" href="#Knet.mat"><code>Knet.mat</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">mat(x)</code></pre><p>Reshape x into a two-dimensional matrix.</p><p>This is typically used when turning the output of a 4-D convolution result into a 2-D input for a fully connected layer.  For 1-D inputs returns <code>reshape(x, (length(x),1))</code>.  For inputs with more than two dimensions of size <code>(X1,X2,...,XD)</code>, returns</p><pre><code class="language-none">reshape(x, (X1*X2*...*X[D-1],XD))</code></pre></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/conv.jl#L347-L360">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.deconv4" href="#Knet.deconv4"><code>Knet.deconv4</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Deconvolution; <code>reverse</code> of convolution.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/conv.jl#L157-L161">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.unpool" href="#Knet.unpool"><code>Knet.unpool</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Unpooling; <code>reverse</code> of pooling.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/conv.jl#L144-L148">source</a><br/></section><h2><a class="nav-anchor" id="Optimization-1" href="#Optimization-1">Optimization</a></h2><p>TODO: need blurb here about how optimization works, need to apply update! to individual weight arrays etc.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.update!" href="#Knet.update!"><code>Knet.update!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>update!</code> takes a weight array w, the gradient (g) of the objective function w.r.t w and an instance of the parameters of an optimization method. It updates w depending on g and the specified optimization method with parameters in-place. If the any parameters of the optimization method need to be updated, it is also updated in-place. The function  returns updated w and the instance of the parameters.</p><p>** Arguments **</p><ul><li><p><code>w</code></p></li><li><p><code>g</code></p></li><li><p><code>prms::T</code></p></li></ul><p>T might be one of the following:</p><pre><code class="language-julia">Sgd
Momentum
Adagrad
Adadelta
Rmsprop
Adam</code></pre><p>** Usage **</p><pre><code class="language-julia">#prms = Sgd()
#prms = Momentum()
#prms = Adagrad()
#prms = Adadelta()
#prms = Rmsprop()
prms = Adam()

w, prms = update!(w, g, prms)
#update!(w, g, prm)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/update.jl#L329-L366">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Sgd" href="#Knet.Sgd"><code>Knet.Sgd</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Stochastic gradient descent is an optimization technique to minimize  an objective function paremeterized by a model&#39;s parameters. It updates the parameters in the opposite direction of the gradient  obtained by taking the gradient of the objective function w.r.t the paremeters. The learning rate (lr) determines the size of the step. It updates the weight with the following formula:</p><pre><code class="language-none">w = w - lr * g</code></pre><p>where w is the weight, g is the gradient of the objective function w.r.t w and lr is the learning rate.</p><p>** Arguments **</p><ul><li><p><code>lr::AbstractFloat=0.001</code></p></li></ul><p>** Usage **</p><p>You can create the Sgd instance with default learning rate value or  you can sepecify the learning rate. Then you can use the created instance in the update! function.</p><pre><code class="language-julia">#prms = Sgd()# use the default value
prms = Sgd(;lr=0.1)
update!(w, g, prms)

#You can change the lr later
prms.lr = 0.01</code></pre></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/update.jl#L1-L34">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Momentum" href="#Knet.Momentum"><code>Knet.Momentum</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Momentum method tries to accelerate the Sgd by adding a velocity term to the update. It also decreases the oscilation between the opposite directions. It updates the weight with the following formulas:</p><pre><code class="language-none">velocity = gamma * velocity + lr * g
w = w - velocity</code></pre><p>where w is the weight, g is the gradient of the objective function w.r.t w, lr is the learning rate, gamma is the momentum parameter,  velocity is an array with the same size and type of w and holds the accelerated gradients.</p><p>** Arguments **</p><ul><li><p><code>lr::AbstractFloat=0.001</code></p></li><li><p><code>gamma::AbstractFloat=0.9</code></p></li><li><p><code>velocity=zeros(w)</code></p></li></ul><p>** Usage ** You can create the Momentum instance with default values or  you can sepecify the parameters. Then you can use the created instance in the update! function.</p><pre><code class="language-julia">#prms = Momentum() # use default values
prms = Momentum(;lr=0.1, gamma=0.95) # generally you do not need to specify the velocity parameter
update!(w, g, prms)

#You can change the parameters later
prms.lr=0.01
prms.gamma=0.9</code></pre><p>Reference:</p><p>Qian, N. (1999). On the momentum term in gradient descent learning algorithms.  Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145–151. http://doi.org/10.1016/S0893-6080(98)00116-6</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/update.jl#L41-L80">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Adagrad" href="#Knet.Adagrad"><code>Knet.Adagrad</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Adagrad is one of the methods that adapts the learning rate to the parameters. It stores the sum of the squares of the gradients to scale the learning rate. The learning rate is adapted for each gradient by the value of current gradient divided by the accumulated gradients. Hence, the learning rate is greater for the parameters where the accumulated gradients are small and the learning rate is small if the accumulated gradients are big. It updates the weight with the following formulas:</p><pre><code class="language-none">G = G + g .^ 2
w = w - g .* lr ./ sqrt(G + eps)</code></pre><p>where w is the weight, g is the gradient of the objective function w.r.t w, lr is the learning rate, G is an array with the same size and type of w and holds the  sum of the squares of the gradients. eps is a small value to prevent the zero value on the denominator.</p><p>** Arguments **</p><ul><li><p><code>lr::AbstractFloat=0.001</code></p></li><li><p><code>eps::AbstractFloat=1e-6</code></p></li><li><p><code>G=zeros(w)</code></p></li></ul><p>** Usage ** You can create the Adagrad instance with default values or  you can sepecify the parameters. Then you can use the created instance in the update! function.</p><pre><code class="language-julia">#prms = Adagrad() # use default values
prms = Adagrad(;lr=0.1) # generally you do not need to specify the G and eps parameters
update!(w, g, prms)

#You can change the parameters later
prms.lr=0.01</code></pre><p>Reference:</p><p>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121–2159. Retrieved from http://jmlr.org/papers/v12/duchi11a.html</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/update.jl#L92-L134">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Adadelta" href="#Knet.Adadelta"><code>Knet.Adadelta</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Adadelta is an extension to Adagrad to preventing the convergence of the  learning rate to zero with increase of time. Adadelta uses two ideas from Momentum and Adagrad. It scales the learning rate based on the accumulated gradients and holds the acceleration term like Momentum. It updates the weight with the following formulas:</p><pre><code class="language-none">G = (1-rho) * g .^ 2 + rho * G
update = g .* sqrt(delta + eps) ./ sqrt(G + eps)
w = w - lr * update
delta = rho * delta + (1-rho) * update .^ 2</code></pre><p>where w is the weight, g is the gradient of the objective function w.r.t w, lr is the learning rate, G is an array with the same size and type of w and holds the  sum of the squares of the gradients. eps is a small value to prevent the zero value on the denominator. rho is the momentum parameter and delta is an array with the same size and type of w and holds the sum of the squared updates.</p><p>** Arguments **</p><ul><li><p><code>lr::AbstractFloat=0.001</code></p></li><li><p><code>rho::AbstractFloat=0.9</code></p></li><li><p><code>eps::AbstractFloat=1e-6</code></p></li><li><p><code>G=zeros(w)</code></p></li><li><p><code>delta=zeros(w)</code></p></li></ul><p>** Usage ** You can create the Adadelta instance with default values or  you can sepecify the parameters. Then you can use the created instance in the update! function.</p><pre><code class="language-julia">#prms = Adadelta() # use default values
prms = Adadelta(;lr=0.1, rho=0.8) # generally you do not need to specify the G, delta and eps parameters
update!(w, g, prms)

#You can change the parameters later
prms.lr=0.01
prms.rho=.9
prms.eps=1-e8</code></pre><p>Reference:</p><p>Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from http://arxiv.org/abs/1212.5701</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/update.jl#L146-L192">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Rmsprop" href="#Knet.Rmsprop"><code>Knet.Rmsprop</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Rmsprop is a similar method to Adadelta that tries to improve Adagrad. It scales the learning rates by dividing the root mean squared of the gradients. It updates the weight with the following formula:</p><pre><code class="language-none">G = (1-rho) * g .^ 2 + rho * G
w = w - lr * g ./ sqrt(G + eps)</code></pre><p>where w is the weight, g is the gradient of the objective function w.r.t w, lr is the learning rate, G is an array with the same size and type of w and holds the  sum of the squares of the gradients. eps is a small value to prevent the zero value on the denominator. rho is the momentum parameter and delta is an array with the same size and type of w and holds the sum of the squared updates.</p><p>** Arguments **</p><ul><li><p><code>lr::AbstractFloat=0.001</code></p></li><li><p><code>rho::AbstractFloat=0.9</code></p></li><li><p><code>eps::AbstractFloat=1e-6</code></p></li><li><p><code>G=zeros(w)</code></p></li></ul><p>** Usage ** You can create the Rmsprop instance with default values or  you can sepecify the parameters. Then you can use the created instance in the update! function.</p><pre><code class="language-julia">#prms = Rmsprop() # use default values
prms = Rmsprop(;lr=0.1, rho=0.9) # generally you do not need to specify the G and eps parameters
update!(w, g, prms)

#You can change the parameters later
prms.lr=0.01
prms.rho=0.8
prms.eps=1-e8</code></pre><p>Reference:</p><p>Tieleman, Tijmen, and Geoffrey Hinton. &quot;Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.&quot;  COURSERA: Neural Networks for Machine Learning 4.2 (2012).</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/update.jl#L206-L249">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Adam" href="#Knet.Adam"><code>Knet.Adam</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Adam is one of the methods that compute the adaptive learning rate. It stores accumulated gradients (first moment) and the sum of the squared of gradients (second). It scales the first and second moment with increase of time. Here is the update formulas:</p><pre><code class="language-none">m = beta1 * m + (1 - beta1) * g
v = beta2 * v + (1 - beta2) * g .* g
mhat = m ./ (1 - beta1 ^ t)
vhat = v ./ (1 - beta2 ^ t)
w = w - (lr / (sqrt(vhat) + eps)) * mhat</code></pre><p>where w is the weight, g is the gradient of the objective function w.r.t w, lr is the learning rate, m is an array with the same size and type of w and holds the  accumulated gradients. v is an array with the same size and type of w and holds the  sum of the squares of the gradients. eps is a small value to prevent the zero value on the denominator. beta1 and beta2 are the parameters to calculate bias corrected first and second moments. t is the update count.</p><p>** Arguments **</p><ul><li><p><code>lr::AbstractFloat=0.001</code></p></li><li><p><code>beta1::AbstractFloat=0.9</code></p></li><li><p><code>beta2::AbstractFloat=0.999</code></p></li><li><p><code>t::AbstractFloat=1</code></p></li><li><p><code>eps::AbstractFloat=1e-8</code></p></li><li><p><code>fstm=zeros(w)</code></p></li><li><p><code>scndm=zeros(w)</code></p></li></ul><p>** Usage ** You can create the Adam instance with default values or  you can sepecify the parameters. Then you can use the created instance in the update! function.</p><pre><code class="language-julia">#prms = Adam() # use default values
prms = Adam(;lr=0.1, beta1=0.95, beta2=0.995) # # generally you do not need to specify the fstm, scndm and eps parameters
update!(w, g, prms)

#You can change the parameters later
prms.lr=0.01
prms.beta1=0.8
prms.beta2=0.9
prms.eps=1e-9</code></pre><p>Reference:</p><p>Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1–13.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/update.jl#L262-L312">source</a><br/></section><h2><a class="nav-anchor" id="Initialization-1" href="#Initialization-1">Initialization</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.gaussian" href="#Knet.gaussian"><code>Knet.gaussian</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">gaussian(a...; mean=0.0, std=0.01)</code></pre><p>Return a Gaussian array with a given mean and standard deviation.  The <code>a</code> arguments are passed to <code>randn</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/distributions.jl#L1-L8">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.xavier" href="#Knet.xavier"><code>Knet.xavier</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">xavier(a...)</code></pre><p>Xavier initialization.  The <code>a</code> arguments are passed to <code>rand</code>.  See (<a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Glorot and Bengio 2010</a>) for a description. <a href="http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1XavierFiller.html#details">Caffe</a> implements this slightly differently. <a href="http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.GlorotUniform">Lasagne</a> calls it <code>GlorotUniform</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/distributions.jl#L13-L25">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.bilinear" href="#Knet.bilinear"><code>Knet.bilinear</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Bilinear interpolation filter weights; used for initializing deconvolution layers.</p><p>Adapted from https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py#L33</p><p>Arguments:</p><p><code>T</code> : Data Type</p><p><code>fw</code>: Width upscale factor</p><p><code>fh</code>: Height upscale factor</p><p><code>IN</code>: Number of input filters</p><p><code>ON</code>: Number of output filters</p><p>Example usage:</p><p>w = bilinear(Float32,2,2,128,128)</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/tree/d53cad5cd72febb37f6c2f9b104cf85cf6762a10/src/distributions.jl#L41-L64">source</a><br/></section><h2><a class="nav-anchor" id="AutoGrad-(advanced)-1" href="#AutoGrad-(advanced)-1">AutoGrad (advanced)</a></h2><p>TODO: blurb here about how AutoGrad works, what the <code>Rec</code> type is etc.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@primitive" href="#AutoGrad.@primitive"><code>AutoGrad.@primitive</code></a> — <span class="docstring-category">Macro</span>.</div><div><p><code>@primitive fx g1 g2...</code> can be used to define a new primitive and (optionally) its gradients.</p><p>Julia supports multiple dispatch, i.e. a single function can have multiple methods with different arg types.  AutoGrad supports multiple dispatch for primitives and gradients.  Thus fx is a typed method declaration such as:</p><ul><li><p>@primitive sin(x::Number)</p></li><li><p>@primitive hypot(x1::Array,x2::Array),dy,y</p></li></ul><p>The second example specifies variable names for the output gradient <code>dy</code> and the output <code>y</code> after the method declaration which can be used in gradient expressions.  Untyped, ellipsis and keyword arguments are ok as in <code>f(a::Int,b,c...;d=1)</code>.  Parametric methods such as <code>f{T&lt;:Number}(x::T)</code> cannot be used.</p><p>The @primitive macro turns the first example into:</p><pre><code class="language-none">local sin_r = recorder(sin)
sin{T&lt;:Number}(x::Rec{T}) = sin_r(x)</code></pre><p>This will cause any call to <code>sin</code> with a Rec{T&lt;:Number} argument to be recorded.  With multiple arguments things are a bit more complicated.  Here is what happens with the second example:</p><pre><code class="language-none">local hypot_r = recorder(hypot)
hypot{T&lt;:Array,S&lt;:Array}(x1::Rec{T},x2::Rec{S})=hypot_r(x1,x2)
hypot{T&lt;:Array,S&lt;:Array}(x1::Rec{T},x2::S)=hypot_r(x1,x2)
hypot{T&lt;:Array,S&lt;:Array}(x1::T,x2::Rec{S})=hypot_r(x1,x2)</code></pre><p>We want the recorder version to be called if any one of the arguments is a boxed Rec.  There is no easy way to specify this in Julia, so the macro generates all 2^N-1 boxed/unboxed argument combinations.</p><p>The method declaration can optionally be followed by gradient expressions.  Here are the same examples with gradients:</p><ul><li><p>@primitive sin(x::Number),dy (dy*cos(x))</p></li><li><p>@primitive hypot(x1::Array,x2::Array),dy,y  <code>(dy.*x1./y)</code>  <code>(dy.*x2./y)</code></p></li></ul><p>Note that the parameters, the return variable and the output gradient of the original function can be used in the gradient expressions.</p><p>In AutoGrad, gradients are defined using gradient methods that have the following signature:</p><pre><code class="language-none">f(Grad{i},dy,y,x...) =&gt; dx[i]</code></pre><p>For the first example here is the generated gradient method:</p><p><code>sin{T&lt;:Number}(::Type{Grad{1}}, dy, y, x::Rec{T})=(dy*cos(x))</code></p><p>For the second example a different gradient method is generated for each argument:</p><p><code>hypot{T&lt;:Array,S&lt;:Array}(::Type{Grad{1}},dy,y,x1::Rec{T},x2::Rec{S})=(dy.*x1./y)</code> <code>hypot{T&lt;:Array,S&lt;:Array}(::Type{Grad{2}},dy,y,x1::Rec{T},x2::Rec{S})=(dy.*x2./y)</code></p><p>In fact @primitive generates four more definitions for the other boxed/unboxed argument combinations.</p><p>Non-differentiable functions such as <code>sign</code>, and non-numeric functions such as <code>size</code> should be defined using the @zerograd macro instead.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/AutoGrad.jl/tree/f7aaac8a664adcb00c9cd9a0673273f297937255/src/util.jl#L12-L79">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@zerograd" href="#AutoGrad.@zerograd"><code>AutoGrad.@zerograd</code></a> — <span class="docstring-category">Macro</span>.</div><div><p><code>@zerograd f(args...; kwargs...)</code> allows f to handle its Rec inputs by unboxing them like @primitive, but unlike @primitive it does not record its actions or return a Rec result.  Some functions, like sign(), have zero gradient.  Others, like length() have discrete or constant outputs.  These need to handle Rec inputs, but do not need to record anything and can return regular values.  Their output can be treated like a constant in the program.  Use the @zerograd macro for those.  Note that kwargs are NOT unboxed.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/AutoGrad.jl/tree/f7aaac8a664adcb00c9cd9a0673273f297937255/src/util.jl#L110-L119">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.getval" href="#AutoGrad.getval"><code>AutoGrad.getval</code></a> — <span class="docstring-category">Function</span>.</div><div><p>getval(x) unboxes x if it is a Rec, otherwise returns x.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/AutoGrad.jl/tree/f7aaac8a664adcb00c9cd9a0673273f297937255/src/core.jl#L141">source</a><br/></section><h2><a class="nav-anchor" id="Function-Index-1" href="#Function-Index-1">Function Index</a></h2><ul><li><a href="reference.html#Knet.Adadelta"><code>Knet.Adadelta</code></a></li><li><a href="reference.html#Knet.Adagrad"><code>Knet.Adagrad</code></a></li><li><a href="reference.html#Knet.Adam"><code>Knet.Adam</code></a></li><li><a href="reference.html#Knet.KnetArray"><code>Knet.KnetArray</code></a></li><li><a href="reference.html#Knet.Momentum"><code>Knet.Momentum</code></a></li><li><a href="reference.html#Knet.Rmsprop"><code>Knet.Rmsprop</code></a></li><li><a href="reference.html#Knet.Sgd"><code>Knet.Sgd</code></a></li><li><a href="reference.html#AutoGrad.getval"><code>AutoGrad.getval</code></a></li><li><a href="reference.html#AutoGrad.grad"><code>AutoGrad.grad</code></a></li><li><a href="reference.html#Knet.bilinear"><code>Knet.bilinear</code></a></li><li><a href="reference.html#Knet.conv4"><code>Knet.conv4</code></a></li><li><a href="reference.html#Knet.cpu2gpu"><code>Knet.cpu2gpu</code></a></li><li><a href="reference.html#Knet.deconv4"><code>Knet.deconv4</code></a></li><li><a href="reference.html#Knet.dir"><code>Knet.dir</code></a></li><li><a href="reference.html#Knet.gaussian"><code>Knet.gaussian</code></a></li><li><a href="reference.html#Knet.gpu"><code>Knet.gpu</code></a></li><li><a href="reference.html#Knet.gpu2cpu"><code>Knet.gpu2cpu</code></a></li><li><a href="reference.html#Knet.invx"><code>Knet.invx</code></a></li><li><a href="reference.html#Knet.logp"><code>Knet.logp</code></a></li><li><a href="reference.html#Knet.logsumexp"><code>Knet.logsumexp</code></a></li><li><a href="reference.html#Knet.mat"><code>Knet.mat</code></a></li><li><a href="reference.html#Knet.pool"><code>Knet.pool</code></a></li><li><a href="reference.html#Knet.relu"><code>Knet.relu</code></a></li><li><a href="reference.html#Knet.sigm"><code>Knet.sigm</code></a></li><li><a href="reference.html#Knet.unpool"><code>Knet.unpool</code></a></li><li><a href="reference.html#Knet.update!"><code>Knet.update!</code></a></li><li><a href="reference.html#Knet.xavier"><code>Knet.xavier</code></a></li><li><a href="reference.html#AutoGrad.@primitive"><code>AutoGrad.@primitive</code></a></li><li><a href="reference.html#AutoGrad.@zerograd"><code>AutoGrad.@zerograd</code></a></li></ul><footer><hr/><a class="previous" href="examples.html"><span class="direction">Previous</span><span class="title">Examples</span></a><a class="next" href="backprop.html"><span class="direction">Next</span><span class="title">Backpropagation</span></a></footer></article></body></html>
