<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · Knet.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/default.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Ubuntu+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="assets/documenter.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="../versions.js"></script></head><body><nav class="toc"><h1>Knet.jl</h1><form class="search" action="search.html"><select id="version-selector" onChange="window.location.href=this.value"><option value="#" selected="selected" disabled="disabled">Version</option></select><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="install.html">Setting up Knet</a></li><li><a class="toctext" href="README.html">Introduction to Knet</a></li><li class="current"><a class="toctext" href="reference.html">Reference</a><ul class="internal"><li><a class="toctext" href="#Function-reference-1">Function reference</a></li><li><a class="toctext" href="#Optimization-methods-1">Optimization methods</a></li><li><a class="toctext" href="#Standard-Library-1">Standard Library</a></li></ul></li></ul></li><li><span class="toctext">Textbook</span><ul><li><a class="toctext" href="backprop.html">Backpropagation</a></li><li><a class="toctext" href="softmax.html">Softmax Classification</a></li><li><a class="toctext" href="mlp.html">Multilayer Perceptrons</a></li><li><a class="toctext" href="cnn.html">Convolutional Neural Networks</a></li><li><a class="toctext" href="rnn.html">Recurrent Neural Networks</a></li><li><a class="toctext" href="rl.html">Reinforcement Learning</a></li><li><a class="toctext" href="opt.html">Optimization</a></li><li><a class="toctext" href="gen.html">Generalization</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li><a href="reference.html">Reference</a></li></ul><a class="edit-page" href="https://github.com/denizyuret/Knet.jl/tree/bb760bcb645f523069f66d1d123144e8524d932f/docs/src/reference.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/></header><h1><a class="nav-anchor" id="Reference-1" href="#Reference-1">Reference</a></h1><h2><a class="nav-anchor" id="Function-reference-1" href="#Function-reference-1">Function reference</a></h2><p>We implement machine learning models in Knet using regular Julia code and the <code>grad</code> function. Knet defines a few more utility functions listed below. See <code>@doc &lt;function&gt;</code> for full details.</p><table><tr><th>function</th><th>description</th></tr><tr><td><code>Knet.dir</code></td><td>returns a path relative to Knet root.</td></tr><tr><td><code>KnetArray</code></td><td>constructs a GPU array.</td></tr><tr><td><code>conv4</code></td><td>executes convolutions or cross-correlations.</td></tr><tr><td><code>cpu2gpu</code></td><td>copies Array to KnetArray</td></tr><tr><td><code>deconv4</code></td><td>reverse of convolution</td></tr><tr><td><code>gpu</code></td><td>determines which GPU Knet uses.</td></tr><tr><td><code>gpu2cpu</code></td><td>copies KnetArray to Array</td></tr><tr><td><code>grad</code></td><td>returns the gradient function.</td></tr><tr><td><code>gradcheck</code></td><td>compares gradients with numeric approximations.</td></tr><tr><td><code>gradloss</code></td><td>version of grad that returns gradient and loss.</td></tr><tr><td><code>invx</code></td><td>returns <code>(1./x)</code></td></tr><tr><td><code>logp</code></td><td>returns <code>x .- log(sum(exp(x),[dims]))</code></td></tr><tr><td><code>logsumexp</code></td><td>returns <code>log(sum(exp(x),[dims]))</code></td></tr><tr><td><code>mat</code></td><td>reshapes its input into a two-dimensional matrix.</td></tr><tr><td><code>pool</code></td><td>replaces several adjacent values with their mean or maximum.</td></tr><tr><td><code>relu</code></td><td>returns <code>max(0,x)</code></td></tr><tr><td><code>sigm</code></td><td>returns <code>(1./(1+exp(-x)))</code></td></tr><tr><td><code>unpool</code></td><td>reverse of pooling</td></tr><tr><td><code>update!</code></td><td>updates the weight depending on the gradient and the parameters of the optimization method</td></tr></table><h2><a class="nav-anchor" id="Optimization-methods-1" href="#Optimization-methods-1">Optimization methods</a></h2><p>In the examples in the introduction, we used simple SGD as the optimization method and performed parameter updates manually using <code>w[i] -= lr * dw[i]</code>. The <code>update!</code> function provides more optimization methods and can be used in place of this manual update. In addition to a weight array <code>w[i]</code> and its gradient <code>dw[i]</code>, <code>update!</code> requires a third argument encapsulating the type, options, and state of the optimization method. The constructors of the supported optimization methods are listed below. See <code>@doc Sgd</code> etc. for full details. Note that in general we need to keep one of these state variables per weight array, see <a href="https://github.com/denizyuret/Knet.jl/blob/master/examples/optimizers.jl">optimizers.jl</a> for example usage.</p><table><tr><th>optimizer</th><th>parameters</th></tr><tr><td><code>Sgd</code></td><td>learning rate</td></tr><tr><td><code>Momentum</code></td><td>learning rate, gamma and velocity</td></tr><tr><td><code>Adam</code></td><td>learning rate, beta1, beta2, epsilon, time, first and second moments</td></tr><tr><td><code>Adagrad</code></td><td>learning rate, epsilon and accumulated gradients (G)</td></tr><tr><td><code>Adadelta</code></td><td>learning rate, rho, epsilon, accumulated gradients (G) and updates (delta)</td></tr><tr><td><code>Rmsprop</code></td><td>learning rate, rho, epsilon and accumulated gradients (G)</td></tr></table><h2><a class="nav-anchor" id="Standard-Library-1" href="#Standard-Library-1">Standard Library</a></h2><p>Julia standard library functions supported by KnetArrays.</p><div class="admonition note"><div class="admonition-title">Unary operators</div><div class="admonition-text"><p>(-),  	  abs,	  abs2,	  acos,	  acosh,	  asin,	  asinh,	  atan,	  atanh,	  cbrt,	  ceil,	  cos,	  cosh,	  cospi,	  erf,	  erfc,	  erfcinv,	  erfcx,	  erfinv,	  exp,	  exp10,	  exp2,	  expm1,	  floor,	  log,	  log10,	  log1p,	  log2,	  round,	  sign,	  sin,	  sinh,	  sinpi,	  sqrt,	  tan,	  tanh,	  trunc</p></div></div><p>Some CUDA math operators not yet supported:</p><div class="admonition note"><div class="admonition-title">Unsupported unary operators</div><div class="admonition-text"><p>cyl_bessel_i0, cyl_bessel_i1, ilogb, j0, j1, lgamma, llrint, llround, logb, lrint, lround, nearbyint, normcdf, normcdfinv, rcbrt, rint, rsqrt, tgamma, y0, y1</p></div></div><p>Broadcasting is supported between arrays of same size, array with scalar, and array with vector (i.e. at most one dimension &gt; 1).</p><div class="admonition note"><div class="admonition-title">Elementwise broadcasting operators</div><div class="admonition-text"><p>(.*), (.+), (.-), (./), (.&lt;), (.&lt;=), (.!=), (.==), (.&gt;), (.&gt;=), (.^), max, min     </p></div></div><p>Note that only Array-&gt;Scalar and Array-&gt;Vector reductions are supported.</p><div class="admonition note"><div class="admonition-title">Reduction operators</div><div class="admonition-text"><p>countnz, maximum, minimum, prod, sum, sumabs, sumabs2, vecnorm</p></div></div><p>Linear algebra uses the CUBLAS library.</p><div class="admonition note"><div class="admonition-title">Linear algebra operators</div><div class="admonition-text"><p>(*), axpy!, permutedims (only 2D and 3D), transpose</p></div></div><footer><hr/><a class="previous" href="README.html"><span class="direction">Previous</span><span class="title">Introduction to Knet</span></a><a class="next" href="backprop.html"><span class="direction">Next</span><span class="title">Backpropagation</span></a></footer></article></body></html>
