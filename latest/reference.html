<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · Knet.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/default.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Ubuntu+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="assets/documenter.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="../versions.js"></script></head><body><nav class="toc"><h1>Knet.jl</h1><form class="search" action="search.html"><select id="version-selector" onChange="window.location.href=this.value"><option value="#" selected="selected" disabled="disabled">Version</option></select><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="install.html">Setting up Knet</a></li><li><a class="toctext" href="README.html">Introduction to Knet</a></li><li class="current"><a class="toctext" href="reference.html">Reference</a><ul class="internal"><li><a class="toctext" href="#Standard-Library-1">Standard Library</a></li><li><a class="toctext" href="#Function-reference-1">Function reference</a></li><li><a class="toctext" href="#Optimization-methods-1">Optimization methods</a></li><li><a class="toctext" href="#Standard-Library-2">Standard Library</a></li><li class="toplevel"><a class="toctext" href="#Currently-unsupported-unary-functions-defined-by-cuda:-1">Currently unsupported unary functions defined by cuda:</a></li><li class="toplevel"><a class="toctext" href="#\"cyl_bessel_i0\",-1">&quot;cyl_bessel_i0&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"cyl_bessel_i1\",-1">&quot;cyl_bessel_i1&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"ilogb\",-1">&quot;ilogb&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"j0\",-1">&quot;j0&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"j1\",-1">&quot;j1&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"lgamma\",-#-missing-digamma-for-derivative-1">&quot;lgamma&quot;, # missing digamma for derivative</a></li><li class="toplevel"><a class="toctext" href="#\"llrint\",-1">&quot;llrint&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"llround\",-1">&quot;llround&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"logb\",-1">&quot;logb&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"lrint\",-1">&quot;lrint&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"lround\",-1">&quot;lround&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"nearbyint\",-1">&quot;nearbyint&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"normcdf\",-1">&quot;normcdf&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"normcdfinv\",-1">&quot;normcdfinv&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"rcbrt\",-1">&quot;rcbrt&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"rint\",-1">&quot;rint&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"rsqrt\",-1">&quot;rsqrt&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"tgamma\",-1">&quot;tgamma&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"y0\",-1">&quot;y0&quot;,</a></li><li class="toplevel"><a class="toctext" href="#\"y1\",-1">&quot;y1&quot;,</a></li></ul></li></ul></li><li><span class="toctext">Textbook</span><ul><li><a class="toctext" href="backprop.html">Backpropagation</a></li><li><a class="toctext" href="softmax.html">Softmax Classification</a></li><li><a class="toctext" href="mlp.html">Multilayer Perceptrons</a></li><li><a class="toctext" href="cnn.html">Convolutional Neural Networks</a></li><li><a class="toctext" href="rnn.html">Recurrent Neural Networks</a></li><li><a class="toctext" href="rl.html">Reinforcement Learning</a></li><li><a class="toctext" href="opt.html">Optimization</a></li><li><a class="toctext" href="gen.html">Generalization</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li><a href="reference.html">Reference</a></li></ul><a class="edit-page" href="https://github.com/denizyuret/Knet.jl/tree/17afcc4efef540be057f5997a1c1cf17f5d5f0a7/docs/src/reference.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/></header><h1><a class="nav-anchor" id="Reference-1" href="#Reference-1">Reference</a></h1><h2><a class="nav-anchor" id="Standard-Library-1" href="#Standard-Library-1">Standard Library</a></h2><p>Julia standard library functions supported by KnetArrays.</p><h2><a class="nav-anchor" id="Function-reference-1" href="#Function-reference-1">Function reference</a></h2><p>We implement machine learning models in Knet using regular Julia code and the <code>grad</code> function. Knet defines a few more utility functions listed below. See <code>@doc &lt;function&gt;</code> for full details.</p><table><tr><th>function</th><th>description</th></tr><tr><td><code>grad</code></td><td>returns the gradient function.</td></tr><tr><td><code>KnetArray</code></td><td>constructs a GPU array.</td></tr><tr><td><code>gradcheck</code></td><td>compares gradients with numeric approximations.</td></tr><tr><td><code>Knet.dir</code></td><td>returns a path relative to Knet root.</td></tr><tr><td><code>gpu</code></td><td>determines which GPU Knet uses.</td></tr><tr><td><code>relu</code></td><td>returns <code>max(0,x)</code></td></tr><tr><td><code>sigm</code></td><td>returns <code>(1./(1+exp(-x)))</code></td></tr><tr><td><code>invx</code></td><td>returns <code>(1./x)</code></td></tr><tr><td><code>logp</code></td><td>returns <code>x .- log(sum(exp(x),[dims]))</code></td></tr><tr><td><code>logsumexp</code></td><td>returns <code>log(sum(exp(x),[dims]))</code></td></tr><tr><td><code>conv4</code></td><td>executes convolutions or cross-correlations.</td></tr><tr><td><code>pool</code></td><td>replaces several adjacent values with their mean or maximum.</td></tr><tr><td><code>mat</code></td><td>reshapes its input into a two-dimensional matrix.</td></tr><tr><td><code>update!</code></td><td>updates the weight depending on the gradient and the parameters of the optimization method</td></tr></table><p>&lt;!–- TODO: move this to reference.md, add @ref to fns, add new fns. –&gt;</p><h2><a class="nav-anchor" id="Optimization-methods-1" href="#Optimization-methods-1">Optimization methods</a></h2><p>In the examples above, we used simple SGD as the optimization method and performed parameter updates manually using <code>w[i] -= lr * dw[i]</code>. The <code>update!</code> function provides more optimization methods and can be used in place of this manual update. In addition to a weight array <code>w[i]</code> and its gradient <code>dw[i]</code>, <code>update!</code> requires a third argument encapsulating the type, options, and state of the optimization method. The constructors of the supported optimization methods are listed below. See <code>@doc Sgd</code> etc. for full details. Note that in general we need to keep one of these state variables per weight array, see <a href="https://github.com/denizyuret/Knet.jl/blob/master/examples/optimizers.jl">optimizers.jl</a> for example usage.</p><table><tr><th>optimizer</th><th>parameters</th></tr><tr><td><code>Sgd</code></td><td>learning rate</td></tr><tr><td><code>Momentum</code></td><td>learning rate, gamma and velocity</td></tr><tr><td><code>Adam</code></td><td>learning rate, beta1, beta2, epsilon, time, first and second moments</td></tr><tr><td><code>Adagrad</code></td><td>learning rate, epsilon and accumulated gradients (G)</td></tr><tr><td><code>Adadelta</code></td><td>learning rate, rho, epsilon, accumulated gradients (G) and updates (delta)</td></tr><tr><td><code>Rmsprop</code></td><td>learning rate, rho, epsilon and accumulated gradients (G)</td></tr></table><h2><a class="nav-anchor" id="Standard-Library-2" href="#Standard-Library-2">Standard Library</a></h2><p>Julia standard library functions supported by KnetArrays.</p><p>.. table:: Unary operations.</p><p>+––––––-+   | -,  	|   | abs,	|   | abs2,	|   | acos,	|   | acosh,	|   | asin,	|   | asinh,	|   | atan,	|   | atanh,	|   | cbrt,	|   | ceil,	|   | cos,	|   | cosh,	|   | cospi,	|   | erf,	|   | erfc,	|   | erfcinv,	|   | erfcx,	|   | erfinv,	|   | exp,	|   | exp10,	|   | exp2,	|   | expm1,	|   | floor,	|   | log,	|   | log10,	|   | log1p,	|   | log2,	|   | round,	|   | sign,	|   | sin,	|   | sinh,	|   | sinpi,	|   | sqrt,	|   | tan,	|   | tanh,	|   | trunc,	|   +––––––-+</p><p>.. TODO link these functions to their Julia docs</p><p>..</p><h1><a class="nav-anchor" id="Currently-unsupported-unary-functions-defined-by-cuda:-1" href="#Currently-unsupported-unary-functions-defined-by-cuda:-1">Currently unsupported unary functions defined by cuda:</a></h1><h1><a class="nav-anchor" id="\"cyl_bessel_i0\",-1" href="#\"cyl_bessel_i0\",-1">&quot;cyl_bessel_i0&quot;,</a></h1><h1><a class="nav-anchor" id="\"cyl_bessel_i1\",-1" href="#\"cyl_bessel_i1\",-1">&quot;cyl_bessel_i1&quot;,</a></h1><h1><a class="nav-anchor" id="\"ilogb\",-1" href="#\"ilogb\",-1">&quot;ilogb&quot;,</a></h1><h1><a class="nav-anchor" id="\"j0\",-1" href="#\"j0\",-1">&quot;j0&quot;,</a></h1><h1><a class="nav-anchor" id="\"j1\",-1" href="#\"j1\",-1">&quot;j1&quot;,</a></h1><h1><a class="nav-anchor" id="\"lgamma\",-#-missing-digamma-for-derivative-1" href="#\"lgamma\",-#-missing-digamma-for-derivative-1">&quot;lgamma&quot;, # missing digamma for derivative</a></h1><h1><a class="nav-anchor" id="\"llrint\",-1" href="#\"llrint\",-1">&quot;llrint&quot;,</a></h1><h1><a class="nav-anchor" id="\"llround\",-1" href="#\"llround\",-1">&quot;llround&quot;,</a></h1><h1><a class="nav-anchor" id="\"logb\",-1" href="#\"logb\",-1">&quot;logb&quot;,</a></h1><h1><a class="nav-anchor" id="\"lrint\",-1" href="#\"lrint\",-1">&quot;lrint&quot;,</a></h1><h1><a class="nav-anchor" id="\"lround\",-1" href="#\"lround\",-1">&quot;lround&quot;,</a></h1><h1><a class="nav-anchor" id="\"nearbyint\",-1" href="#\"nearbyint\",-1">&quot;nearbyint&quot;,</a></h1><h1><a class="nav-anchor" id="\"normcdf\",-1" href="#\"normcdf\",-1">&quot;normcdf&quot;,</a></h1><h1><a class="nav-anchor" id="\"normcdfinv\",-1" href="#\"normcdfinv\",-1">&quot;normcdfinv&quot;,</a></h1><h1><a class="nav-anchor" id="\"rcbrt\",-1" href="#\"rcbrt\",-1">&quot;rcbrt&quot;,</a></h1><h1><a class="nav-anchor" id="\"rint\",-1" href="#\"rint\",-1">&quot;rint&quot;,</a></h1><h1><a class="nav-anchor" id="\"rsqrt\",-1" href="#\"rsqrt\",-1">&quot;rsqrt&quot;,</a></h1><h1><a class="nav-anchor" id="\"tgamma\",-1" href="#\"tgamma\",-1">&quot;tgamma&quot;,</a></h1><h1><a class="nav-anchor" id="\"y0\",-1" href="#\"y0\",-1">&quot;y0&quot;,</a></h1><h1><a class="nav-anchor" id="\"y1\",-1" href="#\"y1\",-1">&quot;y1&quot;,</a></h1><p>..    Unary functions not in standard library    (&quot;invx&quot;, &quot;invx&quot;, &quot;1/xi&quot;),    (&quot;sigm&quot;, &quot;sigm&quot;, &quot;(xi&gt;=0?1/(1+exp(-xi)):(exp(xi)/(1+exp(xi))))&quot;),    (&quot;relu&quot;, &quot;relu&quot;, &quot;(xi&gt;0?xi:0)&quot;),</p><footer><hr/><a class="previous" href="README.html"><span class="direction">Previous</span><span class="title">Introduction to Knet</span></a><a class="next" href="backprop.html"><span class="direction">Next</span><span class="title">Backpropagation</span></a></footer></article></body></html>
