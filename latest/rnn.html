<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Recurrent Neural Networks · Knet.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/default.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Ubuntu+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="assets/documenter.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="../versions.js"></script></head><body><nav class="toc"><h1>Knet.jl</h1><form class="search" action="search.html"><select id="version-selector" onChange="window.location.href=this.value"><option value="#" selected="selected" disabled="disabled">Version</option></select><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="install.html">Setting up Knet</a></li><li><a class="toctext" href="tutorial.html">Introduction to Knet</a></li><li><a class="toctext" href="examples.html">Examples</a></li><li><a class="toctext" href="reference.html">Reference</a></li></ul></li><li><span class="toctext">Textbook</span><ul><li><a class="toctext" href="backprop.html">Backpropagation</a></li><li><a class="toctext" href="softmax.html">Softmax Classification</a></li><li><a class="toctext" href="mlp.html">Multilayer Perceptrons</a></li><li><a class="toctext" href="cnn.html">Convolutional Neural Networks</a></li><li class="current"><a class="toctext" href="rnn.html">Recurrent Neural Networks</a><ul class="internal"><li><a class="toctext" href="#Motivation-1">Motivation</a></li><li><a class="toctext" href="#Architectures-(s2s-vs),-Examples-(lm,mt-vs)-1">Architectures (s2s vs), Examples (lm,mt vs)</a></li><li><a class="toctext" href="#Modules-(lstm-gru-vs)-1">Modules (lstm gru vs)</a></li><li><a class="toctext" href="#Backpropagation-through-time-1">Backpropagation through time</a></li><li><a class="toctext" href="#Practical-concerns-(minibatching,-gclip-etc)-1">Practical concerns (minibatching, gclip etc)</a></li><li><a class="toctext" href="#Code-examples-1">Code examples</a></li><li><a class="toctext" href="#Advanced-1">Advanced</a></li><li><a class="toctext" href="#References-1">References</a></li></ul></li><li><a class="toctext" href="rl.html">Reinforcement Learning</a></li><li><a class="toctext" href="opt.html">Optimization</a></li><li><a class="toctext" href="gen.html">Generalization</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Textbook</li><li><a href="rnn.html">Recurrent Neural Networks</a></li></ul><a class="edit-page" href="https://github.com/denizyuret/Knet.jl/tree/258a9ced8be5a5406b5895f6ff3441ad366a9e63/docs/src/rnn.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/></header><h1><a class="nav-anchor" id="Recurrent-Neural-Networks-1" href="#Recurrent-Neural-Networks-1">Recurrent Neural Networks</a></h1><h2><a class="nav-anchor" id="Motivation-1" href="#Motivation-1">Motivation</a></h2><ul><li><p>fixed size api from karpathy</p></li><li><p>turing completeness, program analogy</p></li><li><p>parameter sharing perspective, goodfellow: compare with 1-D convolution.</p></li><li><p>simple examples with irnn: adding, mnist-by-pixel, lm, timit (do we have data?)</p></li><li><p>other possible examples: postag, charner.</p></li></ul><h2><a class="nav-anchor" id="Architectures-(s2s-vs),-Examples-(lm,mt-vs)-1" href="#Architectures-(s2s-vs),-Examples-(lm,mt-vs)-1">Architectures (s2s vs), Examples (lm,mt vs)</a></h2><p>Modeling sequences: (hinton)</p><ul><li><p>input to output sequence (speech, synched, unsynched, when does output start/stop if unsynched (ctc))</p></li><li><p>predict next token (lm)</p></li><li><p>sequence classification</p></li><li><p>s2s models</p></li><li><p>Karpathy&#39;s graph is more clear</p></li><li><p>Hinton&#39;s providing input and teaching signals variations</p></li><li><p>deeplearningbook 379 (fig 10.3,4,5) has example design patterns</p></li><li><p>graves book chap 2 has a classification, </p></li><li><p>Goodfellow 10.5 Seq-&gt;Tok, 10.9 Tok-&gt;Seq (Tok=Initial and/or Tok=&gt;Input), 10.3,4,10,11 SeqN-&gt;SeqN, Sec 10.4 S2S.</p></li></ul><p>Models: (hinton)</p><ul><li><p>memoryless models, bengios language model</p></li><li><p>start with a regular mlp converted to rnn like Goodfellow.</p></li></ul><h2><a class="nav-anchor" id="Modules-(lstm-gru-vs)-1" href="#Modules-(lstm-gru-vs)-1">Modules (lstm gru vs)</a></h2><ul><li><p>motivation: why do mlp rnns have a hard time learning? vanishing gradients relevant according to (DL 10.7)</p></li><li><p>lstm/gru: http://colah.github.io/posts/2015-08-Understanding-LSTMs/ (DL 10.10)</p></li><li><p>http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/</p></li><li><p>input and output (word Embedding and prediction) layers</p></li></ul><h2><a class="nav-anchor" id="Backpropagation-through-time-1" href="#Backpropagation-through-time-1">Backpropagation through time</a></h2><ul><li><p>Hinton 7b</p></li><li><p>Unfolding picture</p></li></ul><h2><a class="nav-anchor" id="Practical-concerns-(minibatching,-gclip-etc)-1" href="#Practical-concerns-(minibatching,-gclip-etc)-1">Practical concerns (minibatching, gclip etc)</a></h2><ul><li><p>Hinton 7d: why bptt is difficult, back pass linear.</p></li><li><p>http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/</p></li><li><p>Adam and gclip (DL 10.11)</p></li><li><p>minibatching</p></li><li><p>decoding and generating: greedy, beam, stochastic.</p></li></ul><h2><a class="nav-anchor" id="Code-examples-1" href="#Code-examples-1">Code examples</a></h2><h2><a class="nav-anchor" id="Advanced-1" href="#Advanced-1">Advanced</a></h2><ul><li><p>multilayer (DL 10.5)</p></li><li><p>bidirectional</p></li><li><p>attention: http://distill.pub/2016/augmented-rnns/</p></li><li><p>speech, handwriting, mt</p></li><li><p>image captioning, vqa</p></li><li><p>ntm, memory networks: (DL 10.12) http://distill.pub/2016/augmented-rnns/</p></li><li><p>2D rnns: graves chap 8. DL end of 10.3.</p></li><li><p>recursive nets? (DL 10.6)</p></li><li><p>different length input/output sequences: graves a chapter 7 on ctc, chap 6 on hmm hybrids., olah and carter on adaptive computation time. DL 10.4 on s2s.</p></li><li><p>comparison to LDS and HMM (Hinton)</p></li><li><p>discussion of teacher forcing and its potential problems (DL 10.2.1)</p></li><li><p>echo state networks (DL 10.8) just fix the h-&gt;h weights.</p></li><li><p>skip connections in time, leaky units (DL 10.9)</p></li></ul><h2><a class="nav-anchor" id="References-1" href="#References-1">References</a></h2><ul><li><p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy 2015.</a> The Unreasonable Effectiveness of Recurrent Neural Networks.</p></li><li><p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">Olah 2015.</a> Understanding LSTMs.</p></li><li><p><a href="https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec7.pdf">Hinton 2012.</a> RNN lecture slides.</p></li><li><p><a href="http://distill.pub/2016/augmented-rnns">Olah and Carter 2016.</a> Augmented RNNs.</p></li><li><p><a href="http://www.deeplearningbook.org/contents/rnn.html">Goodfellow 2016.</a> Deep Learning Chapter 10. Sequence modeling: recurrent and recursive nets.</p></li><li><p><a href="https://www.cs.toronto.edu/~graves/preprint.pdf">Graves 2012.</a>, Supervised Sequence Labelling with Recurrent Neural Networks (textbook)</p></li><li><p><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns">Britz 2015.</a> Recurrent neural networks tutorial.</p></li><li><p><a href="http://cs224n.stanford.edu/">Manning and Socher 2017.</a> CS224n: Natural Language Processing with Deep Learning.</p></li><li><p><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Wikipedia.</a> Recurrent neural network.</p></li><li><p><a href="https://www.willamette.edu/~gorr/classes/cs449/rnn1.html">Orr 1999.</a> RNN lecture notes.</p></li><li><p><a href="https://arxiv.org/abs/1504.00941">Le et al. 2015.</a> A simple way to initialize recurrent networks of rectified linear units</p></li></ul><footer><hr/><a class="previous" href="cnn.html"><span class="direction">Previous</span><span class="title">Convolutional Neural Networks</span></a><a class="next" href="rl.html"><span class="direction">Next</span><span class="title">Reinforcement Learning</span></a></footer></article></body></html>
